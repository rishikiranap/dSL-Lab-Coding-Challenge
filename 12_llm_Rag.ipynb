{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dslmllab/dSL-Lab-Coding-Challenge/blob/main/12_llm_Rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBUSolhCuwp6"
      },
      "source": [
        "# Large Language Models (LLMs) Tutorial with Challenges\n",
        "\n",
        "## Table of Contents\n",
        "1. Introduction to LLMs\n",
        "2. Key Concepts and Architecture\n",
        "3. Working with Pre-trained Models\n",
        "4. Fine-tuning LLMs\n",
        "5. Prompt Engineering\n",
        "6. LLM Applications\n",
        "7. Challenges and Exercises\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7gshmUmuwp6"
      },
      "source": [
        "## 1. Introduction to LLMs\n",
        "\n",
        "Large Language Models (LLMs) are neural networks trained on vast amounts of text data to understand and generate human-like text. They have revolutionized NLP by achieving state-of-the-art performance on various tasks.\n",
        "\n",
        "### Key Characteristics:\n",
        "- **Scale**: Billions of parameters (GPT-3: 175B, LLaMA: 7B-70B)\n",
        "- **Pre-training**: Trained on massive text corpora\n",
        "- **Transfer Learning**: Can be fine-tuned for specific tasks\n",
        "- **Few-shot Learning**: Can adapt to new tasks with minimal examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stN6U9_huwp6",
        "outputId": "e764e571-cf44-4ca3-8d52-df2657c473bd"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch transformers numpy matplotlib seaborn openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRtR1_TYuwp7",
        "outputId": "8c9a2863-089d-48b4-d4f9-6b1bd2abf718"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f36KCg6uwp7"
      },
      "source": [
        "## 2. Key Concepts and Architecture\n",
        "\n",
        "### Transformer Architecture\n",
        "LLMs are based on the Transformer architecture, which uses self-attention mechanisms to process sequential data.\n",
        "\n",
        "### Key Components:\n",
        "1. **Self-Attention**: Allows the model to focus on different parts of the input\n",
        "2. **Positional Encoding**: Provides position information to the model\n",
        "3. **Feed-Forward Networks**: Process the attention outputs\n",
        "4. **Layer Normalization**: Stabilizes training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmfpiZS2uwp7",
        "outputId": "4bb82fc9-4cb5-449b-bdfe-22704e44bca9"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (    AutoTokenizer,     AutoModelForCausalLM,    pipeline,    GPT2LMHeadModel,    GPT2Tokenizer)\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check if GPU is available\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Note: If you get import errors, install required packages:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngRRSZjguwp7"
      },
      "source": [
        "## 3. Working with Pre-trained Models\n",
        "\n",
        "Let's load and use a pre-trained language model from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqLteNeFuwp7",
        "outputId": "db77de01-a211-4195-eddc-d10b6d7ac922"
      },
      "outputs": [],
      "source": [
        "# Load a small pre-trained model (GPT-2)\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model = model.to(device)\n",
        "\n",
        "# Set pad token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3ySV0sPuwp7",
        "outputId": "58b1fb7b-f329-4601-d6d6-c9433e297c60"
      },
      "outputs": [],
      "source": [
        "# Text generation function\n",
        "def generate_text(prompt, max_length=50, temperature=0.8, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Generate text using the loaded model\n",
        "\n",
        "    Args:\n",
        "        prompt: Input text prompt\n",
        "        max_length: Maximum length of generated text (including prompt)\n",
        "        temperature: Controls randomness (0.0 = deterministic, 1.0 = random)\n",
        "        top_p: Nucleus sampling parameter\n",
        "    \"\"\"\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Calculate prompt length\n",
        "    prompt_length = inputs['input_ids'].shape[1]\n",
        "\n",
        "    # Ensure max_length is greater than prompt length\n",
        "    effective_max_length = max(max_length, prompt_length + 50)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,  # Use max_new_tokens instead of max_length\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and return\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Test generation\n",
        "prompt = \"The future of artificial intelligence is\"\n",
        "generated = generate_text(prompt, max_length=50)\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Generated: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IilSu1uOuwp7"
      },
      "source": [
        "## 4. Fine-tuning LLMs\n",
        "\n",
        "Fine-tuning allows us to adapt pre-trained models to specific tasks or domains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNFAKEl7uwp8",
        "outputId": "717e2c9a-f9d3-47db-e7b7-67ff2f45ff20"
      },
      "outputs": [],
      "source": [
        "# Example: Preparing data for fine-tuning\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze()\n",
        "        }\n",
        "\n",
        "# Sample training data\n",
        "training_texts = [\n",
        "    \"Machine learning is transforming industries.\",\n",
        "    \"Natural language processing enables computers to understand human language.\",\n",
        "    \"Deep learning models can learn complex patterns from data.\",\n",
        "    \"Transformers have revolutionized NLP tasks.\"\n",
        "]\n",
        "\n",
        "# Create dataset\n",
        "dataset = TextDataset(training_texts, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Batch example shape: {next(iter(dataloader))['input_ids'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aXzXsJKuwp8",
        "outputId": "7e8114c5-3318-4443-aa07-316f594042f3"
      },
      "outputs": [],
      "source": [
        "# Simple fine-tuning loop (demonstration)\n",
        "from torch.optim import AdamW\n",
        "\n",
        "def train_step(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Move to device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=input_ids\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Train for one epoch (demonstration)\n",
        "print(\"Training for 1 epoch...\")\n",
        "avg_loss = train_step(model, dataloader, optimizer, device)\n",
        "print(f\"Average loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96K036wwuwp8"
      },
      "source": [
        "## 5. Prompt Engineering\n",
        "\n",
        "Prompt engineering is the art of crafting inputs to get desired outputs from LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fq6Sj5iAuwp8",
        "outputId": "7d238c49-74b1-4bfb-fb08-e64b410e23e9"
      },
      "outputs": [],
      "source": [
        "# Prompt engineering examples\n",
        "class PromptTemplates:\n",
        "    @staticmethod\n",
        "    def zero_shot(task, input_text):\n",
        "        return f\"{task}: {input_text}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def few_shot(task, examples, input_text):\n",
        "        prompt = f\"{task}\\n\\n\"\n",
        "        for ex in examples:\n",
        "            prompt += f\"Input: {ex['input']}\\nOutput: {ex['output']}\\n\\n\"\n",
        "        prompt += f\"Input: {input_text}\\nOutput:\"\n",
        "        return prompt\n",
        "\n",
        "    @staticmethod\n",
        "    def chain_of_thought(question):\n",
        "        return f\"{question}\\n\\nLet's think step by step:\"\n",
        "\n",
        "# Example: Sentiment analysis with few-shot learning\n",
        "sentiment_examples = [\n",
        "    {\"input\": \"This movie was fantastic!\", \"output\": \"Positive\"},\n",
        "    {\"input\": \"I really didn't enjoy the food.\", \"output\": \"Negative\"},\n",
        "    {\"input\": \"The weather is okay today.\", \"output\": \"Neutral\"}\n",
        "]\n",
        "\n",
        "test_text = \"The service was excellent and the staff were friendly.\"\n",
        "prompt = PromptTemplates.few_shot(\n",
        "    \"Classify the sentiment of the following text\",\n",
        "    sentiment_examples,\n",
        "    test_text\n",
        ")\n",
        "\n",
        "print(\"Few-shot prompt:\")\n",
        "print(prompt)\n",
        "print(\"\\nModel output:\")\n",
        "# Use max_new_tokens for better control\n",
        "print(generate_text(prompt, temperature=0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UotxhAFZuwp8"
      },
      "source": [
        "## 6. LLM Applications\n",
        "\n",
        "Let's explore some practical applications of LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VlmCBwVuwp8",
        "outputId": "c027dd2a-3844-4584-f7c0-dedd8d568c47"
      },
      "outputs": [],
      "source": [
        "# Application 1: Text Summarization\n",
        "def summarize_text(text, model, tokenizer, max_summary_length=50):\n",
        "    prompt = f\"Summarize the following text in one sentence:\\n\\n{text}\\n\\nSummary:\"\n",
        "    return generate_text(prompt, max_length=len(prompt.split()) + max_summary_length)\n",
        "\n",
        "# Example\n",
        "long_text = \"\"\"\n",
        "Artificial intelligence has made significant strides in recent years,\n",
        "particularly in the field of natural language processing. Large language\n",
        "models like GPT, BERT, and T5 have demonstrated remarkable capabilities\n",
        "in understanding and generating human-like text. These models are trained\n",
        "on vast amounts of data and can perform various tasks such as translation,\n",
        "summarization, and question answering without task-specific training.\n",
        "\"\"\"\n",
        "\n",
        "summary = summarize_text(long_text, model, tokenizer)\n",
        "print(\"Original text:\")\n",
        "print(long_text)\n",
        "print(\"\\nSummary:\")\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yr_Ra68Huwp8",
        "outputId": "cc565799-d141-4909-d8f5-4849943d52ca"
      },
      "outputs": [],
      "source": [
        "# Application 2: Code Generation\n",
        "def generate_code(description):\n",
        "    prompt = f\"# Python function that {description}\\ndef\"\n",
        "    return generate_text(prompt, max_length=150, temperature=0.2)\n",
        "\n",
        "# Example\n",
        "code_description = \"calculates the factorial of a number recursively\"\n",
        "generated_code = generate_code(code_description)\n",
        "print(\"Generated code:\")\n",
        "print(generated_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ragEN7u2uwp8",
        "outputId": "aec4b2d8-af8b-4b33-b4e7-0781e2593b7f"
      },
      "outputs": [],
      "source": [
        "# Application 3: Question Answering\n",
        "def answer_question(context, question):\n",
        "    prompt = f\"\"\"Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    return generate_text(prompt, max_length=len(prompt.split()) + 50, temperature=0.3)\n",
        "\n",
        "# Example\n",
        "context = \"\"\"The Transformer architecture was introduced in the paper 'Attention is All You Need'\n",
        "by Vaswani et al. in 2017. It revolutionized NLP by replacing recurrent layers with\n",
        "self-attention mechanisms, allowing for better parallelization and capturing long-range dependencies.\"\"\"\n",
        "\n",
        "question = \"When was the Transformer architecture introduced?\"\n",
        "answer = answer_question(context, question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCsEr1bSuwp8"
      },
      "source": [
        "## 7. Challenges and Exercises\n",
        "\n",
        "Now it's time to test your understanding with these challenges!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYziIzfUuwp8"
      },
      "source": [
        "### Challenge 1: Implement Temperature Sampling\n",
        "\n",
        "Implement a function that demonstrates how temperature affects text generation. Generate the same prompt with different temperature values and compare the outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLBQxainuwp8"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "def generate_with_temp(prompt, temperature):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        temperature=temperature,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        do_sample=True\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "def compare_temperatures_local(prompt, temperatures=[0.1, 0.5, 1.0, 1.5]):\n",
        "    for temp in temperatures:\n",
        "        text = generate_with_temp(prompt, temp)\n",
        "        print(f\"\\n--- Temperature: {temp} ---\\n{text}\\n\")\n",
        "\n",
        "compare_temperatures_local(\"The meaning of life is\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS5YVHV-uwp8"
      },
      "source": [
        "### Challenge 2: Build a Custom Few-Shot Classifier\n",
        "\n",
        "Create a few-shot classifier for a custom task (e.g., classifying programming languages from code snippets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MGqRRN4uwp8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "class FewShotClassifier:\n",
        "    def __init__(self, model, tokenizer, device=None):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.examples = []\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def add_example(self, input_text, label):\n",
        "        \"\"\"\n",
        "        Store input-label pairs to build the few-shot prompt\n",
        "        \"\"\"\n",
        "        self.examples.append((input_text.strip(), label.strip()))\n",
        "\n",
        "    def build_prompt(self, input_text):\n",
        "        \"\"\"\n",
        "        Creating a prompt with few-shot examples and the query input\n",
        "        \"\"\"\n",
        "        prompt = \"Classify the programming language of the following code snippets:\\n\\n\"\n",
        "        for example_text, label in self.examples:\n",
        "            prompt += f\"Code: {example_text}\\nLanguage: {label}\\n\\n\"\n",
        "        prompt += f\"Code: {input_text.strip()}\\nLanguage:\"\n",
        "        return prompt\n",
        "\n",
        "    def classify(self, input_text, max_new_tokens=10):\n",
        "        \"\"\"\n",
        "        Run inference using the language model to predict the label\n",
        "        \"\"\"\n",
        "        prompt = self.build_prompt(input_text)\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        prediction = generated_text.split(\"Language:\")[-1].strip().split(\"\\n\")[0]\n",
        "        return prediction\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load a causal language model and tokenizer\n",
        "    model_name = \"Salesforce/codegen-350M-mono\"  # You can replace with another code model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    # Initialize classifier\n",
        "    classifier = FewShotClassifier(model, tokenizer)\n",
        "\n",
        "    # Add few-shot examples\n",
        "    classifier.add_example(\"def greet():\\n    print('Hello')\", \"Python\")\n",
        "    classifier.add_example(\"console.log('Hello');\", \"JavaScript\")\n",
        "    classifier.add_example(\"std::cout << 'Hello';\", \"C++\")\n",
        "\n",
        "    # Test input\n",
        "    test_code = \"print('This is a test to detect which language it is')\"\n",
        "    predicted_language = classifier.classify(test_code)\n",
        "\n",
        "    print(\"\\nInput Code:\\n\", test_code)\n",
        "    print(\"Predicted Language:\", predicted_language)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDvd9imEuwp8"
      },
      "source": [
        "### Challenge 3: Implement Beam Search\n",
        "\n",
        "Implement beam search decoding for text generation and compare it with greedy decoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERPUEBzRuwp8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def beam_search_generate(model, tokenizer, prompt, beam_width=3, max_length=50):\n",
        "    device = model.device\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Tokenize the prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    # Initialize beams with (sequence, score)\n",
        "    beams = [(input_ids, 0)]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        new_beams = []\n",
        "\n",
        "        for seq, score in beams:\n",
        "            # Stop if already ended with EOS token\n",
        "            if seq[0, -1].item() == tokenizer.eos_token_id:\n",
        "                new_beams.append((seq, score))\n",
        "                continue\n",
        "\n",
        "            # Get model predictions\n",
        "            with torch.no_grad():\n",
        "                outputs = model(seq)\n",
        "                logits = outputs.logits[:, -1, :]  # Get logits for last token\n",
        "\n",
        "            probs = torch.log_softmax(logits, dim=-1)  # Convert to log-probs\n",
        "            topk_probs, topk_ids = torch.topk(probs, beam_width, dim=-1)\n",
        "\n",
        "            # Expand each beam\n",
        "            for i in range(beam_width):\n",
        "                next_token = topk_ids[0, i].unsqueeze(0).unsqueeze(0)\n",
        "                new_seq = torch.cat([seq, next_token], dim=1)\n",
        "                new_score = score + topk_probs[0, i].item()\n",
        "                new_beams.append((new_seq, new_score))\n",
        "\n",
        "        # Keep top-k beams\n",
        "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "        # Early stopping if all beams ended with EOS\n",
        "        if all(seq[0, -1].item() == tokenizer.eos_token_id for seq, _ in beams):\n",
        "            break\n",
        "\n",
        "    # Return best sequence\n",
        "    best_seq = beams[0][0][0]\n",
        "    return tokenizer.decode(best_seq, skip_special_tokens=True)\n",
        "\n",
        "prompt = \"The future of technology\"\n",
        "beam_output = beam_search_generate(model, tokenizer, prompt, beam_width=3, max_length=50)\n",
        "print(\"Beam Search Output:\", beam_output)\n",
        "\n",
        "greedy_output = model.generate(tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device), \n",
        "                               max_length=50, temperature=0.0)\n",
        "print(\"Greedy Output:\", tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyP-lOlAuwp8"
      },
      "source": [
        "### Challenge 4: Prompt Optimization\n",
        "\n",
        "Design and test different prompt templates for a specific task (e.g., translation, style transfer) and evaluate which works best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rOPo5Bzwuwp9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'best_template': 'Convert the following formal sentence to casual English:\\n\\n\"{input}\"', 'average_score': 3.0}\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def evaluate_prompts(task_description, test_cases, prompt_templates):\n",
        "    \"\"\"\n",
        "    Evaluate different prompt templates for a task.\n",
        "\n",
        "    Args:\n",
        "        task_description (str): Description of the NLP task.\n",
        "        test_cases (list): List of input strings (e.g., formal sentences).\n",
        "        prompt_templates (list): List of prompt templates with a {input} placeholder.\n",
        "\n",
        "    Returns:\n",
        "        dict: Best-performing template and its average score.\n",
        "    \"\"\"\n",
        "    def mock_model_response(prompt):\n",
        "        \"\"\"\n",
        "        Simulates LLM output. \n",
        "        \"\"\"\n",
        "        # Simple rule-based simulation of \"formal to casual\"\n",
        "        prompt_text = re.sub(r\".*?:\", \"\", prompt).strip()\n",
        "        prompt_text = prompt_text.replace(\"I would like to request\", \"Can you help me\")\n",
        "        prompt_text = prompt_text.replace(\"Please find the attached document\", \"Here's the doc\")\n",
        "        prompt_text = prompt_text.replace(\"for your review\", \"for you to check out\")\n",
        "        return prompt_text\n",
        "\n",
        "    def score_output(output):\n",
        "        \"\"\"\n",
        "        Scores the output based on how casual it sounds.\n",
        "        Uses presence of contractions and casual phrasing.\n",
        "        \"\"\"\n",
        "        casual_phrases = [\"gonna\", \"wanna\", \"here's\", \"can you\", \"help me\", \"doc\", \"check out\"]\n",
        "        contractions = [\"can't\", \"won't\", \"don't\", \"it's\", \"I'm\", \"you're\", \"they're\", \"here's\"]\n",
        "        score = sum(1 for phrase in casual_phrases + contractions if phrase in output.lower())\n",
        "        return score\n",
        "\n",
        "    results = []\n",
        "    for template in prompt_templates:\n",
        "        total_score = 0\n",
        "        for test in test_cases:\n",
        "            prompt = template.format(input=test)\n",
        "            output = mock_model_response(prompt)\n",
        "            score = score_output(output)\n",
        "            total_score += score\n",
        "        avg_score = total_score / len(test_cases)\n",
        "        results.append((template, avg_score))\n",
        "\n",
        "    # Find best-performing template\n",
        "    best_template, best_score = max(results, key=lambda x: x[1])\n",
        "    return {\n",
        "        \"best_template\": best_template,\n",
        "        \"average_score\": best_score\n",
        "    }\n",
        "\n",
        "# --- Example usage ---\n",
        "task_description = \"Convert formal English to casual English\"\n",
        "\n",
        "test_cases = [\n",
        "    \"I would like to request your assistance with this matter.\",\n",
        "    \"Please find the attached document for your review.\"\n",
        "]\n",
        "\n",
        "prompt_templates = [\n",
        "    # Template 1\n",
        "    \"Convert the following formal sentence to casual English:\\n\\n\\\"{input}\\\"\",\n",
        "    \n",
        "    # Template 2\n",
        "    \"Make this sentence sound more casual:\\n\\n{input}\",\n",
        "    \n",
        "    # Template 3\n",
        "    \"Rewrite this in informal tone:\\n\\n'{input}'\"\n",
        "]\n",
        "\n",
        "result = evaluate_prompts(task_description, test_cases, prompt_templates)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlpKrODouwp9"
      },
      "source": [
        "### Challenge 5: Build a Simple RAG System\n",
        "\n",
        "Implement a basic Retrieval-Augmented Generation (RAG) system that retrieves relevant context before generating answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpH6W-xvuwp9"
      },
      "outputs": [],
      "source": [
        "# Challenge 5: Your code here\n",
        "class SimpleRAG:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.knowledge_base = []\n",
        "\n",
        "    def add_document(self, document):\n",
        "        \"\"\"\n",
        "        Add a document to the knowledge base\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "    def retrieve(self, query, k=3):\n",
        "        \"\"\"\n",
        "        Retrieve top-k relevant documents for the query\n",
        "\n",
        "        TODO:\n",
        "        1. Implement a simple similarity metric (e.g., word overlap)\n",
        "        2. Return top-k most relevant documents\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "    def generate_answer(self, query):\n",
        "        \"\"\"\n",
        "        Generate an answer using retrieved context\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "# Test your RAG system\n",
        "# rag = SimpleRAG(model, tokenizer)\n",
        "# Add some documents and test question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yDaksuRuwp9"
      },
      "source": [
        "### Challenge 6: Analyze Model Biases\n",
        "\n",
        "Design experiments to test for potential biases in the language model and propose mitigation strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ranf50bSuwp9"
      },
      "outputs": [],
      "source": [
        "# Challenge 7: Your code here\n",
        "def calculate_perplexity(model, tokenizer, text_corpus):\n",
        "    \"\"\"\n",
        "    Calculate perplexity of the model on a text corpus\n",
        "\n",
        "    Perplexity = exp(average negative log-likelihood)\n",
        "\n",
        "    TODO:\n",
        "    1. Tokenize the text corpus\n",
        "    2. Calculate log probabilities for each token\n",
        "    3. Compute average negative log-likelihood\n",
        "    4. Return perplexity\n",
        "    \"\"\"\n",
        "    # Your implementation here\n",
        "    pass\n",
        "\n",
        "# Test on sample texts\n",
        "# sample_texts = [\n",
        "#     \"The quick brown fox jumps over the lazy dog.\",\n",
        "#     \"Machine learning is a subset of artificial intelligence.\",\n",
        "#     \"asdfjkl qwerty zxcvbn\"  # Random text for comparison\n",
        "# ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjmiQUK2uwp9"
      },
      "source": [
        "### Challenge 8: Create a Dialogue System\n",
        "\n",
        "Build a simple dialogue system that maintains context across multiple turns of conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "O64ske6suwp9"
      },
      "outputs": [],
      "source": [
        "# Challenge 8: Your code here\n",
        "class DialogueSystem:\n",
        "    def __init__(self, model, tokenizer, max_history=5):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_history = max_history\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def add_turn(self, speaker, text):\n",
        "        \"\"\"\n",
        "        Add a conversation turn to history\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "    def generate_response(self, user_input):\n",
        "        \"\"\"\n",
        "        Generate a response considering conversation history\n",
        "\n",
        "        TODO:\n",
        "        1. Format conversation history as context\n",
        "        2. Create appropriate prompt\n",
        "        3. Generate response\n",
        "        4. Update conversation history\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "    def reset_conversation(self):\n",
        "        \"\"\"Reset conversation history\"\"\"\n",
        "        self.conversation_history = []\n",
        "\n",
        "# Test the dialogue system\n",
        "# dialogue = DialogueSystem(model, tokenizer)\n",
        "# Simulate a multi-turn conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IccvyVFEuwp9"
      },
      "source": [
        "## Bonus Challenges\n",
        "\n",
        "### Advanced Challenge 1: Implement LoRA (Low-Rank Adaptation)\n",
        "Research and implement a simple version of LoRA for efficient fine-tuning.\n",
        "\n",
        "### Advanced Challenge 2: Build a Token Prediction Visualizer\n",
        "Create a visualization tool that shows the top-k predicted tokens at each generation step.\n",
        "\n",
        "### Advanced Challenge 3: Implement Constrained Generation\n",
        "Build a system that generates text with constraints (e.g., must include certain words, follow a specific pattern)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV08oyCMuwp9"
      },
      "source": [
        "## Resources for Further Learning\n",
        "\n",
        "1. **Papers to Read:**\n",
        "   - \"Attention Is All You Need\" (Vaswani et al., 2017)\n",
        "   - \"Language Models are Few-Shot Learners\" (GPT-3 paper)\n",
        "   - \"BERT: Pre-training of Deep Bidirectional Transformers\"\n",
        "\n",
        "2. **Useful Libraries:**\n",
        "   - Hugging Face Transformers\n",
        "   - LangChain for LLM applications\n",
        "   - OpenAI API for GPT models\n",
        "\n",
        "3. **Online Resources:**\n",
        "   - Hugging Face Course\n",
        "   - Fast.ai Practical Deep Learning\n",
        "   - The Illustrated Transformer\n",
        "\n",
        "4. **Practice Platforms:**\n",
        "   - Kaggle NLP competitions\n",
        "   - Hugging Face Model Hub\n",
        "   - Papers with Code"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
