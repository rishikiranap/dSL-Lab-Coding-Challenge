{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering Systems\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "At the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand different types of question answering systems\n",
    "2. Implement rule-based and retrieval-based QA systems\n",
    "3. Build reading comprehension models\n",
    "4. Create knowledge-based question answering\n",
    "5. Implement conversational QA systems\n",
    "6. Handle multi-hop reasoning questions\n",
    "7. Evaluate QA system performance\n",
    "8. Deploy QA systems for production use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Question Answering\n",
    "\n",
    "Question Answering (QA) is a computer science discipline that focuses on building systems that automatically answer questions posed by humans in natural language. QA systems combine techniques from information retrieval, natural language processing, and machine learning.\n",
    "\n",
    "### Types of QA Systems:\n",
    "\n",
    "1. **Factoid QA**: Answer specific factual questions (\"Who is the president of France?\")\n",
    "2. **Reading Comprehension**: Answer questions based on given passages\n",
    "3. **Open-domain QA**: Answer questions using large knowledge bases\n",
    "4. **Conversational QA**: Multi-turn question answering in dialogue\n",
    "5. **Visual QA**: Answer questions about images\n",
    "6. **Multi-hop QA**: Questions requiring reasoning across multiple facts\n",
    "\n",
    "### QA System Architecture:\n",
    "\n",
    "1. **Question Analysis**: Parse and understand the question\n",
    "2. **Document Retrieval**: Find relevant documents/passages\n",
    "3. **Answer Extraction**: Extract or generate the answer\n",
    "4. **Answer Ranking**: Score and rank potential answers\n",
    "5. **Response Generation**: Format the final response\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- **Search Engines**: Direct answers to search queries\n",
    "- **Chatbots**: Customer service and virtual assistants\n",
    "- **Educational Tools**: Tutoring and learning systems\n",
    "- **Information Systems**: Enterprise knowledge bases\n",
    "- **Healthcare**: Medical diagnosis assistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages",
    "!pip install numpy pandas matplotlib seaborn nltk spacy scikit-learn torch transformers tqdm networkx",
    "",
    "# Download spaCy English model if not present",
    "import spacy",
    "try:",
    "    spacy.load(\"en_core_web_sm\")",
    "except OSError:",
    "    !python -m spacy download en_core_web_sm",
    "",
    "# Download required NLTK data",
    "import nltk",
    "for item in ['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words']:",
    "    nltk.download(item, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Deep learning imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# NLTK downloads\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import ne_chunk, pos_tag\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rule-Based Question Answering\n",
    "\n",
    "Starting with a simple rule-based approach using patterns and templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedQA:\n",
    "    def __init__(self):\n",
    "        # Question patterns and their corresponding answer extractors\n",
    "        self.question_patterns = {\n",
    "            'who': {\n",
    "                'patterns': [r'who is', r'who was', r'who are', r'who were'],\n",
    "                'answer_type': 'PERSON',\n",
    "                'extraction_method': 'named_entity'\n",
    "            },\n",
    "            'what': {\n",
    "                'patterns': [r'what is', r'what was', r'what are', r'what were', r'what does'],\n",
    "                'answer_type': 'DEFINITION',\n",
    "                'extraction_method': 'definition'\n",
    "            },\n",
    "            'where': {\n",
    "                'patterns': [r'where is', r'where was', r'where are', r'where were'],\n",
    "                'answer_type': 'LOCATION',\n",
    "                'extraction_method': 'named_entity'\n",
    "            },\n",
    "            'when': {\n",
    "                'patterns': [r'when is', r'when was', r'when did', r'when will'],\n",
    "                'answer_type': 'TIME',\n",
    "                'extraction_method': 'temporal'\n",
    "            },\n",
    "            'how': {\n",
    "                'patterns': [r'how many', r'how much', r'how long', r'how often'],\n",
    "                'answer_type': 'QUANTITY',\n",
    "                'extraction_method': 'numerical'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Simple knowledge base\n",
    "        self.knowledge_base = {\n",
    "            'facts': [\n",
    "                \"Albert Einstein was a German-born theoretical physicist.\",\n",
    "                \"Paris is the capital of France.\",\n",
    "                \"The Eiffel Tower is located in Paris, France.\",\n",
    "                \"World War II ended in 1945.\",\n",
    "                \"Shakespeare wrote Romeo and Juliet.\",\n",
    "                \"The Great Wall of China is approximately 13,000 miles long.\",\n",
    "                \"Marie Curie won Nobel prizes in Physics and Chemistry.\",\n",
    "                \"The Amazon River is the longest river in the world.\",\n",
    "                \"DNA stands for Deoxyribonucleic Acid.\",\n",
    "                \"Mount Everest is the highest mountain in the world.\"\n",
    "            ],\n",
    "            'definitions': {\n",
    "                'artificial intelligence': 'AI is the simulation of human intelligence in machines.',\n",
    "                'machine learning': 'ML is a method of data analysis that automates analytical model building.',\n",
    "                'natural language processing': 'NLP is a branch of AI that helps computers understand human language.',\n",
    "                'deep learning': 'Deep learning is a subset of ML based on artificial neural networks.'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def analyze_question(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze question to determine type and extract key information\"\"\"\n",
    "        question_lower = question.lower().strip()\n",
    "        \n",
    "        # Determine question type\n",
    "        question_type = None\n",
    "        for q_type, info in self.question_patterns.items():\n",
    "            for pattern in info['patterns']:\n",
    "                if re.search(pattern, question_lower):\n",
    "                    question_type = q_type\n",
    "                    break\n",
    "            if question_type:\n",
    "                break\n",
    "        \n",
    "        # Extract keywords\n",
    "        tokens = word_tokenize(question_lower)\n",
    "        keywords = [self.lemmatizer.lemmatize(token) for token in tokens \n",
    "                   if token.isalpha() and token not in self.stop_words]\n",
    "        \n",
    "        return {\n",
    "            'type': question_type,\n",
    "            'keywords': keywords,\n",
    "            'original': question\n",
    "        }\n",
    "    \n",
    "    def extract_named_entities(self, text: str, entity_type: str) -> List[str]:\n",
    "        \"\"\"Extract named entities of specified type\"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        named_entities = ne_chunk(pos_tags)\n",
    "        \n",
    "        entities = []\n",
    "        for chunk in named_entities:\n",
    "            if hasattr(chunk, 'label') and chunk.label() == entity_type:\n",
    "                entity_name = ' '.join([token for token, pos in chunk.leaves()])\n",
    "                entities.append(entity_name)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def find_relevant_facts(self, keywords: List[str], top_k: int = 3) -> List[str]:\n",
    "        \"\"\"Find relevant facts from knowledge base\"\"\"\n",
    "        relevant_facts = []\n",
    "        \n",
    "        for fact in self.knowledge_base['facts']:\n",
    "            fact_lower = fact.lower()\n",
    "            score = sum(1 for keyword in keywords if keyword in fact_lower)\n",
    "            if score > 0:\n",
    "                relevant_facts.append((fact, score))\n",
    "        \n",
    "        # Sort by relevance and return top-k\n",
    "        relevant_facts.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [fact for fact, score in relevant_facts[:top_k]]\n",
    "    \n",
    "    def extract_answer(self, question_info: Dict[str, Any], relevant_facts: List[str]) -> str:\n",
    "        \"\"\"Extract answer based on question type and relevant facts\"\"\"\n",
    "        if not relevant_facts:\n",
    "            return \"I don't have enough information to answer that question.\"\n",
    "        \n",
    "        question_type = question_info['type']\n",
    "        keywords = question_info['keywords']\n",
    "        \n",
    "        if question_type == 'who':\n",
    "            # Look for person names\n",
    "            for fact in relevant_facts:\n",
    "                persons = self.extract_named_entities(fact, 'PERSON')\n",
    "                if persons:\n",
    "                    return persons[0]\n",
    "        \n",
    "        elif question_type == 'where':\n",
    "            # Look for locations\n",
    "            for fact in relevant_facts:\n",
    "                locations = self.extract_named_entities(fact, 'GPE')  # Geopolitical entity\n",
    "                if locations:\n",
    "                    return locations[0]\n",
    "                # Simple pattern matching for location indicators\n",
    "                if ' in ' in fact.lower():\n",
    "                    parts = fact.lower().split(' in ')\n",
    "                    if len(parts) > 1:\n",
    "                        return parts[1].split('.')[0].strip().title()\n",
    "        \n",
    "        elif question_type == 'when':\n",
    "            # Look for temporal expressions\n",
    "            for fact in relevant_facts:\n",
    "                # Simple regex for years\n",
    "                years = re.findall(r'\\b(19|20)\\d{2}\\b', fact)\n",
    "                if years:\n",
    "                    return years[0]\n",
    "        \n",
    "        elif question_type == 'what':\n",
    "            # Check definitions first\n",
    "            for keyword in keywords:\n",
    "                if keyword in self.knowledge_base['definitions']:\n",
    "                    return self.knowledge_base['definitions'][keyword]\n",
    "            \n",
    "            # Return most relevant fact\n",
    "            return relevant_facts[0]\n",
    "        \n",
    "        elif question_type == 'how':\n",
    "            # Look for numerical answers\n",
    "            for fact in relevant_facts:\n",
    "                numbers = re.findall(r'\\b\\d+[,\\d]*\\b', fact)\n",
    "                if numbers:\n",
    "                    return numbers[0]\n",
    "        \n",
    "        # Default: return most relevant fact\n",
    "        return relevant_facts[0]\n",
    "    \n",
    "    def answer_question(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main method to answer a question\"\"\"\n",
    "        # Analyze question\n",
    "        question_info = self.analyze_question(question)\n",
    "        \n",
    "        # Find relevant facts\n",
    "        relevant_facts = self.find_relevant_facts(question_info['keywords'])\n",
    "        \n",
    "        # Extract answer\n",
    "        answer = self.extract_answer(question_info, relevant_facts)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'question_type': question_info['type'],\n",
    "            'keywords': question_info['keywords'],\n",
    "            'relevant_facts': relevant_facts,\n",
    "            'answer': answer\n",
    "        }\n",
    "\n",
    "# Initialize and test rule-based QA\n",
    "rule_qa = RuleBasedQA()\n",
    "\n",
    "# Test questions\n",
    "test_questions = [\n",
    "    \"Who was Albert Einstein?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Where is the Eiffel Tower located?\",\n",
    "    \"When did World War II end?\",\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How long is the Great Wall of China?\",\n",
    "    \"Who wrote Romeo and Juliet?\"\n",
    "]\n",
    "\n",
    "print(\"Rule-Based Question Answering Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for question in test_questions:\n",
    "    result = rule_qa.answer_question(question)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"Type: {result['question_type']}\")\n",
    "    print(f\"Keywords: {result['keywords']}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieval-Based Question Answering\n",
    "\n",
    "Implementing a more sophisticated approach using document retrieval and similarity matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalBasedQA:\n",
    "    def __init__(self, corpus: List[str]):\n",
    "        self.corpus = corpus\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            max_features=1000,\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        self.document_vectors = None\n",
    "        self.build_index()\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build TF-IDF index for the corpus\"\"\"\n",
    "        print(f\"Building index for {len(self.corpus)} documents...\")\n",
    "        self.document_vectors = self.vectorizer.fit_transform(self.corpus)\n",
    "        print(\"Index built successfully!\")\n",
    "    \n",
    "    def retrieve_documents(self, question: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Retrieve top-k most relevant documents for the question\"\"\"\n",
    "        # Vectorize question\n",
    "        question_vector = self.vectorizer.transform([question])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(question_vector, self.document_vectors).flatten()\n",
    "        \n",
    "        # Get top-k documents\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        retrieved_docs = []\n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] > 0:  # Only return documents with positive similarity\n",
    "                retrieved_docs.append((self.corpus[idx], similarities[idx]))\n",
    "        \n",
    "        return retrieved_docs\n",
    "    \n",
    "    def extract_answer_from_passage(self, question: str, passage: str) -> str:\n",
    "        \"\"\"Extract answer from a passage using simple heuristics\"\"\"\n",
    "        # Split passage into sentences\n",
    "        sentences = sent_tokenize(passage)\n",
    "        \n",
    "        # Find sentence most similar to question\n",
    "        if len(sentences) == 1:\n",
    "            return sentences[0]\n",
    "        \n",
    "        # Vectorize question and sentences\n",
    "        all_texts = [question] + sentences\n",
    "        vectors = self.vectorizer.transform(all_texts)\n",
    "        \n",
    "        # Calculate similarities between question and each sentence\n",
    "        question_vector = vectors[0:1]\n",
    "        sentence_vectors = vectors[1:]\n",
    "        \n",
    "        similarities = cosine_similarity(question_vector, sentence_vectors).flatten()\n",
    "        \n",
    "        # Return most similar sentence\n",
    "        best_sentence_idx = np.argmax(similarities)\n",
    "        return sentences[best_sentence_idx]\n",
    "    \n",
    "    def answer_question(self, question: str, top_k: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Answer question using retrieval-based approach\"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        retrieved_docs = self.retrieve_documents(question, top_k)\n",
    "        \n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                'question': question,\n",
    "                'answer': \"I couldn't find relevant information to answer your question.\",\n",
    "                'confidence': 0.0,\n",
    "                'source_documents': []\n",
    "            }\n",
    "        \n",
    "        # Extract answer from most relevant document\n",
    "        best_passage, confidence = retrieved_docs[0]\n",
    "        answer = self.extract_answer_from_passage(question, best_passage)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'confidence': confidence,\n",
    "            'source_documents': [doc for doc, score in retrieved_docs]\n",
    "        }\n",
    "\n",
    "# Create a larger corpus for retrieval-based QA\n",
    "retrieval_corpus = [\n",
    "    \"Albert Einstein was a German-born theoretical physicist who developed the theory of relativity. He received the Nobel Prize in Physics in 1921.\",\n",
    "    \"Paris is the capital and most populous city of France. It is known for landmarks like the Eiffel Tower and the Louvre Museum.\",\n",
    "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It was completed in 1889 and stands 324 meters tall.\",\n",
    "    \"World War II was a global war that lasted from 1939 to 1945. It ended with the surrender of Japan in September 1945.\",\n",
    "    \"William Shakespeare was an English playwright and poet. He wrote famous plays including Romeo and Juliet, Hamlet, and Macbeth.\",\n",
    "    \"The Great Wall of China is a series of fortifications built across northern China. It stretches approximately 13,000 miles in total length.\",\n",
    "    \"Marie Curie was a Polish-French physicist and chemist. She was the first woman to win a Nobel Prize and the first person to win Nobel Prizes in two different sciences.\",\n",
    "    \"The Amazon River is the longest river in the world, flowing approximately 4,000 miles through South America from Peru to Brazil.\",\n",
    "    \"DNA, or Deoxyribonucleic Acid, is the hereditary material in humans and almost all other organisms. It contains genetic instructions for development and function.\",\n",
    "    \"Mount Everest is Earth's highest mountain above sea level, located in the Himalayas between Nepal and Tibet. It stands 8,848.86 meters tall.\",\n",
    "    \"Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, and self-correction.\",\n",
    "    \"Machine Learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data.\",\n",
    "    \"The Roman Empire was one of the largest empires in ancient history. At its peak, it controlled territory from Britain to the Middle East and North Africa.\",\n",
    "    \"Leonardo da Vinci was an Italian Renaissance polymath known for his paintings, inventions, and scientific observations. His most famous works include the Mona Lisa and The Last Supper.\",\n",
    "    \"The Pacific Ocean is the largest and deepest ocean on Earth. It covers about one-third of the Earth's surface and contains more than half of the free water on Earth.\"\n",
    "]\n",
    "\n",
    "# Initialize retrieval-based QA\n",
    "retrieval_qa = RetrievalBasedQA(retrieval_corpus)\n",
    "\n",
    "# Test questions\n",
    "retrieval_test_questions = [\n",
    "    \"Who was Albert Einstein and what did he achieve?\",\n",
    "    \"What is the height of Mount Everest?\",\n",
    "    \"When was the Eiffel Tower completed?\",\n",
    "    \"What is the length of the Amazon River?\",\n",
    "    \"Who painted the Mona Lisa?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Which ocean is the largest?\"\n",
    "]\n",
    "\n",
    "print(\"\\nRetrieval-Based Question Answering Results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for question in retrieval_test_questions:\n",
    "    result = retrieval_qa.answer_question(question)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "    print(f\"Source: {result['source_documents'][0][:100]}...\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reading Comprehension QA\n",
    "\n",
    "Building a system that can answer questions based on a given passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadingComprehensionQA:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 3))\n",
    "        \n",
    "        # Question type patterns\n",
    "        self.question_types = {\n",
    "            'who': r'^who\\b',\n",
    "            'what': r'^what\\b',\n",
    "            'where': r'^where\\b',\n",
    "            'when': r'^when\\b',\n",
    "            'why': r'^why\\b',\n",
    "            'how': r'^how\\b',\n",
    "            'which': r'^which\\b',\n",
    "            'yes_no': r'^(is|are|was|were|do|does|did|can|could|will|would)\\b'\n",
    "        }\n",
    "    \n",
    "    def identify_question_type(self, question: str) -> str:\n",
    "        \"\"\"Identify the type of question\"\"\"\n",
    "        question_lower = question.lower().strip()\n",
    "        \n",
    "        for q_type, pattern in self.question_types.items():\n",
    "            if re.match(pattern, question_lower):\n",
    "                return q_type\n",
    "        \n",
    "        return 'unknown'\n",
    "    \n",
    "    def extract_keywords(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract important keywords from text\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Filter out stop words and non-alphabetic tokens\n",
    "        keywords = [token for token in tokens \n",
    "                   if token.isalpha() and token not in stop_words and len(token) > 2]\n",
    "        \n",
    "        return keywords\n",
    "    \n",
    "    def find_answer_candidates(self, passage: str, question: str, question_type: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find potential answer spans in the passage\"\"\"\n",
    "        sentences = sent_tokenize(passage)\n",
    "        question_keywords = set(self.extract_keywords(question))\n",
    "        \n",
    "        candidates = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_keywords = set(self.extract_keywords(sentence))\n",
    "            \n",
    "            # Calculate keyword overlap\n",
    "            overlap = len(question_keywords.intersection(sentence_keywords))\n",
    "            overlap_score = overlap / len(question_keywords) if question_keywords else 0\n",
    "            \n",
    "            # Calculate semantic similarity using TF-IDF\n",
    "            try:\n",
    "                vectors = self.vectorizer.fit_transform([question, sentence])\n",
    "                similarity = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
    "            except:\n",
    "                similarity = 0\n",
    "            \n",
    "            # Combined score\n",
    "            combined_score = 0.6 * similarity + 0.4 * overlap_score\n",
    "            \n",
    "            if combined_score > 0.1:  # Threshold for relevance\n",
    "                candidates.append((sentence, combined_score))\n",
    "        \n",
    "        # Sort by score\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        return candidates\n",
    "    \n",
    "    def extract_specific_answer(self, sentence: str, question: str, question_type: str) -> str:\n",
    "        \"\"\"Extract specific answer from sentence based on question type\"\"\"\n",
    "        if question_type == 'yes_no':\n",
    "            # For yes/no questions, return yes or no based on context\n",
    "            return \"Yes\" if sentence else \"No\"\n",
    "        \n",
    "        elif question_type in ['who', 'what', 'where', 'when', 'why', 'how', 'which']:\n",
    "            # For wh-questions, try to extract specific entities or phrases\n",
    "            \n",
    "            if question_type == 'who':\n",
    "                # Look for person names (capitalized words)\n",
    "                words = sentence.split()\n",
    "                names = [word for word in words if word[0].isupper() and word.isalpha()]\n",
    "                if names:\n",
    "                    return ' '.join(names[:2])  # Return first two capitalized words\n",
    "            \n",
    "            elif question_type == 'when':\n",
    "                # Look for temporal expressions\n",
    "                time_patterns = [\n",
    "                    r'\\b(19|20)\\d{2}\\b',  # Years\n",
    "                    r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\b',  # Months\n",
    "                    r'\\b\\d{1,2}\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{4}\\b'  # Dates\n",
    "                ]\n",
    "                \n",
    "                for pattern in time_patterns:\n",
    "                    matches = re.findall(pattern, sentence, re.IGNORECASE)\n",
    "                    if matches:\n",
    "                        return matches[0] if isinstance(matches[0], str) else ' '.join(matches[0])\n",
    "            \n",
    "            elif question_type == 'where':\n",
    "                # Look for location indicators\n",
    "                if ' in ' in sentence:\n",
    "                    parts = sentence.split(' in ')\n",
    "                    if len(parts) > 1:\n",
    "                        location = parts[1].split(',')[0].split('.')[0].strip()\n",
    "                        return location\n",
    "            \n",
    "            elif question_type == 'how':\n",
    "                # Look for quantities or measurements\n",
    "                quantity_patterns = [\n",
    "                    r'\\b\\d+[,\\d]*\\s*(meters?|feet|miles?|kilometers?|years?|hours?|minutes?)\\b',\n",
    "                    r'\\b\\d+[,\\d]*\\b'\n",
    "                ]\n",
    "                \n",
    "                for pattern in quantity_patterns:\n",
    "                    matches = re.findall(pattern, sentence, re.IGNORECASE)\n",
    "                    if matches:\n",
    "                        return matches[0]\n",
    "        \n",
    "        # If no specific extraction, return the whole sentence\n",
    "        return sentence\n",
    "    \n",
    "    def answer_question(self, passage: str, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Answer question based on given passage\"\"\"\n",
    "        # Identify question type\n",
    "        question_type = self.identify_question_type(question)\n",
    "        \n",
    "        # Find answer candidates\n",
    "        candidates = self.find_answer_candidates(passage, question, question_type)\n",
    "        \n",
    "        if not candidates:\n",
    "            return {\n",
    "                'question': question,\n",
    "                'answer': \"I cannot find an answer to this question in the given passage.\",\n",
    "                'confidence': 0.0,\n",
    "                'question_type': question_type,\n",
    "                'source_sentence': None\n",
    "            }\n",
    "        \n",
    "        # Get best candidate\n",
    "        best_sentence, confidence = candidates[0]\n",
    "        \n",
    "        # Extract specific answer\n",
    "        answer = self.extract_specific_answer(best_sentence, question, question_type)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'confidence': confidence,\n",
    "            'question_type': question_type,\n",
    "            'source_sentence': best_sentence\n",
    "        }\n",
    "\n",
    "# Initialize reading comprehension QA\n",
    "rc_qa = ReadingComprehensionQA()\n",
    "\n",
    "# Test passage and questions\n",
    "test_passage = \"\"\"\n",
    "Albert Einstein was born on March 14, 1879, in Ulm, Germany. He was a theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics. Einstein received the Nobel Prize in Physics in 1921 for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect. He published more than 300 scientific papers and 150 non-scientific works. Einstein is widely regarded as one of the greatest physicists of all time. He died on April 18, 1955, in Princeton, New Jersey, at the age of 76.\n",
    "\"\"\"\n",
    "\n",
    "rc_test_questions = [\n",
    "    \"When was Albert Einstein born?\",\n",
    "    \"Where was Einstein born?\",\n",
    "    \"What did Einstein develop?\",\n",
    "    \"When did Einstein receive the Nobel Prize?\",\n",
    "    \"How many scientific papers did Einstein publish?\",\n",
    "    \"Where did Einstein die?\",\n",
    "    \"Was Einstein a physicist?\"\n",
    "]\n",
    "\n",
    "print(\"\\nReading Comprehension QA Results:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Passage: {test_passage.strip()}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for question in rc_test_questions:\n",
    "    result = rc_qa.answer_question(test_passage, question)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(f\"Type: {result['question_type']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Reading Comprehension Model\n",
    "\n",
    "Implementing a simple neural network for reading comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, contexts, questions, answers, vocab, max_length=200):\n",
    "        self.contexts = contexts\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def text_to_indices(self, text):\n",
    "        \"\"\"Convert text to indices using vocabulary\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(indices) < self.max_length:\n",
    "            indices.extend([self.vocab['<PAD>']] * (self.max_length - len(indices)))\n",
    "        else:\n",
    "            indices = indices[:self.max_length]\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        context = self.contexts[idx]\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "        \n",
    "        # Convert to indices\n",
    "        context_indices = self.text_to_indices(context)\n",
    "        question_indices = self.text_to_indices(question)\n",
    "        \n",
    "        # For simplicity, we'll treat this as a classification problem\n",
    "        # In a real scenario, you'd have start/end positions for the answer\n",
    "        answer_class = hash(answer) % 100  # Simple hash-based classification\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(context_indices, dtype=torch.long),\n",
    "            torch.tensor(question_indices, dtype=torch.long),\n",
    "            torch.tensor(answer_class, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "class NeuralQAModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_classes=100):\n",
    "        super(NeuralQAModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.question_lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=8, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim * 4, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, context, question):\n",
    "        # Embed inputs\n",
    "        context_emb = self.embedding(context)  # (batch_size, seq_len, embedding_dim)\n",
    "        question_emb = self.embedding(question)\n",
    "        \n",
    "        # LSTM encoding\n",
    "        context_encoded, _ = self.context_lstm(context_emb)  # (batch_size, seq_len, hidden_dim*2)\n",
    "        question_encoded, _ = self.question_lstm(question_emb)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attended_context, _ = self.attention(question_encoded, context_encoded, context_encoded)\n",
    "        \n",
    "        # Pool representations\n",
    "        context_pooled = torch.mean(context_encoded, dim=1)  # (batch_size, hidden_dim*2)\n",
    "        question_pooled = torch.mean(attended_context, dim=1)\n",
    "        \n",
    "        # Combine representations\n",
    "        combined = torch.cat([context_pooled, question_pooled], dim=1)  # (batch_size, hidden_dim*4)\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(combined)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class NeuralReadingComprehension:\n",
    "    def __init__(self):\n",
    "        self.vocab = None\n",
    "        self.model = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def build_vocabulary(self, texts, min_freq=2):\n",
    "        \"\"\"Build vocabulary from texts\"\"\"\n",
    "        word_freq = Counter()\n",
    "        \n",
    "        for text in texts:\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            word_freq.update(tokens)\n",
    "        \n",
    "        # Create vocabulary\n",
    "        vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        idx = 2\n",
    "        \n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= min_freq:\n",
    "                vocab[word] = idx\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        return vocab\n",
    "    \n",
    "    def create_training_data(self):\n",
    "        \"\"\"Create synthetic training data for demonstration\"\"\"\n",
    "        contexts = [\n",
    "            \"Albert Einstein was born in Germany in 1879. He developed the theory of relativity.\",\n",
    "            \"Paris is the capital of France. It is known for the Eiffel Tower.\",\n",
    "            \"The Pacific Ocean is the largest ocean in the world. It covers one-third of Earth's surface.\",\n",
    "            \"Shakespeare wrote many famous plays including Romeo and Juliet.\",\n",
    "            \"The Great Wall of China is approximately 13,000 miles long.\"\n",
    "        ]\n",
    "        \n",
    "        questions = [\n",
    "            \"Where was Einstein born?\",\n",
    "            \"What is the capital of France?\",\n",
    "            \"Which ocean is the largest?\",\n",
    "            \"Who wrote Romeo and Juliet?\",\n",
    "            \"How long is the Great Wall?\"\n",
    "        ]\n",
    "        \n",
    "        answers = [\n",
    "            \"Germany\",\n",
    "            \"Paris\",\n",
    "            \"Pacific Ocean\",\n",
    "            \"Shakespeare\",\n",
    "            \"13,000 miles\"\n",
    "        ]\n",
    "        \n",
    "        return contexts, questions, answers\n",
    "    \n",
    "    def train_model(self, contexts, questions, answers, epochs=10):\n",
    "        \"\"\"Train the neural QA model\"\"\"\n",
    "        # Build vocabulary\n",
    "        all_texts = contexts + questions + answers\n",
    "        self.build_vocabulary(all_texts)\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = QADataset(contexts, questions, answers, self.vocab)\n",
    "        dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        vocab_size = len(self.vocab)\n",
    "        self.model = NeuralQAModel(vocab_size).to(self.device)\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            self.model.train()\n",
    "            \n",
    "            for context, question, answer in dataloader:\n",
    "                context = context.to(self.device)\n",
    "                question = question.to(self.device)\n",
    "                answer = answer.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(context, question)\n",
    "                loss = criterion(outputs, answer)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    def predict(self, context, question):\n",
    "        \"\"\"Predict answer for given context and question\"\"\"\n",
    "        if self.model is None:\n",
    "            return \"Model not trained yet.\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Create dummy dataset for prediction\n",
    "        dummy_answer = \"dummy\"\n",
    "        dataset = QADataset([context], [question], [dummy_answer], self.vocab)\n",
    "        context_tensor, question_tensor, _ = dataset[0]\n",
    "        \n",
    "        # Add batch dimension\n",
    "        context_tensor = context_tensor.unsqueeze(0).to(self.device)\n",
    "        question_tensor = question_tensor.unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(context_tensor, question_tensor)\n",
    "            predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "        \n",
    "        return f\"Predicted class: {predicted_class} (This is a simplified demo)\"\n",
    "\n",
    "# Demonstrate neural QA (simplified version)\n",
    "neural_qa = NeuralReadingComprehension()\n",
    "\n",
    "print(\"\\nNeural Reading Comprehension Demo:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create training data\n",
    "contexts, questions, answers = neural_qa.create_training_data()\n",
    "\n",
    "print(f\"Training on {len(contexts)} examples...\")\n",
    "neural_qa.train_model(contexts, questions, answers, epochs=5)\n",
    "\n",
    "# Test prediction\n",
    "test_context = \"Leonardo da Vinci was an Italian artist born in 1452. He painted the Mona Lisa.\"\n",
    "test_question = \"Who painted the Mona Lisa?\"\n",
    "\n",
    "prediction = neural_qa.predict(test_context, test_question)\n",
    "print(f\"\\nTest Context: {test_context}\")\n",
    "print(f\"Test Question: {test_question}\")\n",
    "print(f\"Prediction: {prediction}\")\n",
    "\n",
    "print(\"\\nNote: This is a simplified neural QA model for demonstration.\")\n",
    "print(\"Production models like BERT-based QA systems are much more sophisticated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conversational QA System\n",
    "\n",
    "Building a multi-turn conversation system that maintains context across questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationalQA:\n",
    "    def __init__(self, knowledge_base: List[str]):\n",
    "        self.knowledge_base = knowledge_base\n",
    "        self.conversation_history = []\n",
    "        self.context_entities = set()\n",
    "        self.current_topic = None\n",
    "        \n",
    "        # Initialize components\n",
    "        self.retrieval_qa = RetrievalBasedQA(knowledge_base)\n",
    "        self.rc_qa = ReadingComprehensionQA()\n",
    "        \n",
    "        # Coreference resolution patterns\n",
    "        self.pronouns = {'he', 'she', 'it', 'they', 'him', 'her', 'them', 'his', 'hers', 'its', 'their'}\n",
    "        self.demonstratives = {'this', 'that', 'these', 'those'}\n",
    "    \n",
    "    def extract_entities(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract named entities from text\"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        named_entities = ne_chunk(pos_tags)\n",
    "        \n",
    "        entities = []\n",
    "        for chunk in named_entities:\n",
    "            if hasattr(chunk, 'label'):\n",
    "                entity_name = ' '.join([token for token, pos in chunk.leaves()])\n",
    "                entities.append(entity_name)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def resolve_coreferences(self, question: str) -> str:\n",
    "        \"\"\"Simple coreference resolution using conversation history\"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return question\n",
    "        \n",
    "        words = word_tokenize(question.lower())\n",
    "        resolved_question = question\n",
    "        \n",
    "        # Replace pronouns and demonstratives with entities from context\n",
    "        for word in words:\n",
    "            if word in self.pronouns or word in self.demonstratives:\n",
    "                if self.context_entities:\n",
    "                    # Use the most recent entity as replacement\n",
    "                    recent_entity = list(self.context_entities)[-1]\n",
    "                    resolved_question = resolved_question.replace(word, recent_entity, 1)\n",
    "        \n",
    "        return resolved_question\n",
    "    \n",
    "    def update_context(self, question: str, answer: str):\n",
    "        \"\"\"Update conversation context with new information\"\"\"\n",
    "        # Extract entities from question and answer\n",
    "        question_entities = self.extract_entities(question)\n",
    "        answer_entities = self.extract_entities(answer)\n",
    "        \n",
    "        # Update context entities\n",
    "        self.context_entities.update(question_entities)\n",
    "        self.context_entities.update(answer_entities)\n",
    "        \n",
    "        # Keep only recent entities to avoid context pollution\n",
    "        if len(self.context_entities) > 10:\n",
    "            self.context_entities = set(list(self.context_entities)[-10:])\n",
    "        \n",
    "        # Update conversation history\n",
    "        self.conversation_history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'entities': question_entities + answer_entities\n",
    "        })\n",
    "        \n",
    "        # Keep only recent history\n",
    "        if len(self.conversation_history) > 5:\n",
    "            self.conversation_history = self.conversation_history[-5:]\n",
    "    \n",
    "    def get_conversation_context(self) -> str:\n",
    "        \"\"\"Get relevant context from conversation history\"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return \"\"\n",
    "        \n",
    "        context_parts = []\n",
    "        for turn in self.conversation_history[-3:]:  # Last 3 turns\n",
    "            context_parts.append(f\"Q: {turn['question']} A: {turn['answer']}\")\n",
    "        \n",
    "        return \" \".join(context_parts)\n",
    "    \n",
    "    def ask_question(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a question in conversational context\"\"\"\n",
    "        original_question = question\n",
    "        \n",
    "        # Resolve coreferences\n",
    "        resolved_question = self.resolve_coreferences(question)\n",
    "        \n",
    "        # Get conversation context\n",
    "        context = self.get_conversation_context()\n",
    "        \n",
    "        # Enhance question with context if needed\n",
    "        if context and len(resolved_question.split()) < 5:  # Short questions might need context\n",
    "            enhanced_question = f\"{context} {resolved_question}\"\n",
    "        else:\n",
    "            enhanced_question = resolved_question\n",
    "        \n",
    "        # Get answer using retrieval-based QA\n",
    "        qa_result = self.retrieval_qa.answer_question(enhanced_question)\n",
    "        answer = qa_result['answer']\n",
    "        confidence = qa_result['confidence']\n",
    "        \n",
    "        # Update context\n",
    "        self.update_context(original_question, answer)\n",
    "        \n",
    "        return {\n",
    "            'original_question': original_question,\n",
    "            'resolved_question': resolved_question,\n",
    "            'enhanced_question': enhanced_question,\n",
    "            'answer': answer,\n",
    "            'confidence': confidence,\n",
    "            'context_entities': list(self.context_entities),\n",
    "            'conversation_length': len(self.conversation_history)\n",
    "        }\n",
    "    \n",
    "    def reset_conversation(self):\n",
    "        \"\"\"Reset conversation state\"\"\"\n",
    "        self.conversation_history = []\n",
    "        self.context_entities = set()\n",
    "        self.current_topic = None\n",
    "    \n",
    "    def get_conversation_summary(self) -> str:\n",
    "        \"\"\"Get a summary of the conversation\"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return \"No conversation yet.\"\n",
    "        \n",
    "        summary = f\"Conversation with {len(self.conversation_history)} turns.\\n\"\n",
    "        summary += f\"Entities discussed: {', '.join(self.context_entities)}\\n\"\n",
    "        summary += \"Recent questions:\\n\"\n",
    "        \n",
    "        for i, turn in enumerate(self.conversation_history[-3:], 1):\n",
    "            summary += f\"{i}. {turn['question']}\\n\"\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Initialize conversational QA\n",
    "conv_qa = ConversationalQA(retrieval_corpus)\n",
    "\n",
    "# Simulate a conversation\n",
    "conversation_questions = [\n",
    "    \"Who was Albert Einstein?\",\n",
    "    \"When was he born?\",\n",
    "    \"What did he develop?\",\n",
    "    \"When did he receive the Nobel Prize?\",\n",
    "    \"Where did he die?\"\n",
    "]\n",
    "\n",
    "print(\"\\nConversational QA Demo:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(conversation_questions, 1):\n",
    "    result = conv_qa.ask_question(question)\n",
    "    \n",
    "    print(f\"Turn {i}:\")\n",
    "    print(f\"  User: {result['original_question']}\")\n",
    "    if result['resolved_question'] != result['original_question']:\n",
    "        print(f\"  Resolved: {result['resolved_question']}\")\n",
    "    print(f\"  Bot: {result['answer']}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.3f}\")\n",
    "    print(f\"  Context entities: {result['context_entities']}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nConversation Summary:\")\n",
    "print(conv_qa.get_conversation_summary())\n",
    "\n",
    "# Test coreference resolution\n",
    "print(\"\\nTesting coreference resolution:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "conv_qa.reset_conversation()\n",
    "conv_qa.ask_question(\"Tell me about Marie Curie.\")\n",
    "result = conv_qa.ask_question(\"What did she accomplish?\")\n",
    "\n",
    "print(f\"Original: What did she accomplish?\")\n",
    "print(f\"Resolved: {result['resolved_question']}\")\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. QA System Evaluation\n",
    "\n",
    "Implementing comprehensive evaluation metrics for QA systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAEvaluator:\n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def exact_match(self, predicted: str, ground_truth: str) -> float:\n",
    "        \"\"\"Calculate exact match score\"\"\"\n",
    "        predicted_clean = predicted.strip().lower()\n",
    "        ground_truth_clean = ground_truth.strip().lower()\n",
    "        return 1.0 if predicted_clean == ground_truth_clean else 0.0\n",
    "    \n",
    "    def token_overlap_f1(self, predicted: str, ground_truth: str) -> float:\n",
    "        \"\"\"Calculate F1 score based on token overlap\"\"\"\n",
    "        pred_tokens = set(word_tokenize(predicted.lower()))\n",
    "        true_tokens = set(word_tokenize(ground_truth.lower()))\n",
    "        \n",
    "        if not pred_tokens and not true_tokens:\n",
    "            return 1.0\n",
    "        if not pred_tokens or not true_tokens:\n",
    "            return 0.0\n",
    "        \n",
    "        overlap = pred_tokens.intersection(true_tokens)\n",
    "        precision = len(overlap) / len(pred_tokens)\n",
    "        recall = len(overlap) / len(true_tokens)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        return f1\n",
    "    \n",
    "    def semantic_similarity(self, predicted: str, ground_truth: str) -> float:\n",
    "        \"\"\"Calculate semantic similarity using TF-IDF cosine similarity\"\"\"\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            vectors = vectorizer.fit_transform([predicted, ground_truth])\n",
    "            similarity = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
    "            return similarity\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def answer_relevance(self, predicted: str, question: str) -> float:\n",
    "        \"\"\"Calculate how relevant the answer is to the question\"\"\"\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(stop_words='english')\n",
    "            vectors = vectorizer.fit_transform([predicted, question])\n",
    "            relevance = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
    "            return relevance\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_single(self, predicted: str, ground_truth: str, question: str) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate a single QA prediction\"\"\"\n",
    "        return {\n",
    "            'exact_match': self.exact_match(predicted, ground_truth),\n",
    "            'token_f1': self.token_overlap_f1(predicted, ground_truth),\n",
    "            'semantic_similarity': self.semantic_similarity(predicted, ground_truth),\n",
    "            'answer_relevance': self.answer_relevance(predicted, question)\n",
    "        }\n",
    "    \n",
    "    def evaluate_dataset(self, predictions: List[str], ground_truths: List[str], \n",
    "                        questions: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate entire dataset\"\"\"\n",
    "        if len(predictions) != len(ground_truths) or len(predictions) != len(questions):\n",
    "            raise ValueError(\"All lists must have the same length\")\n",
    "        \n",
    "        all_metrics = defaultdict(list)\n",
    "        \n",
    "        for pred, truth, question in zip(predictions, ground_truths, questions):\n",
    "            metrics = self.evaluate_single(pred, truth, question)\n",
    "            for metric, value in metrics.items():\n",
    "                all_metrics[metric].append(value)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_metrics = {}\n",
    "        for metric, values in all_metrics.items():\n",
    "            avg_metrics[f'avg_{metric}'] = np.mean(values)\n",
    "            avg_metrics[f'std_{metric}'] = np.std(values)\n",
    "        \n",
    "        return avg_metrics\n",
    "    \n",
    "    def compare_systems(self, system_results: Dict[str, List[str]], \n",
    "                       ground_truths: List[str], questions: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Compare multiple QA systems\"\"\"\n",
    "        comparison_data = []\n",
    "        \n",
    "        for system_name, predictions in system_results.items():\n",
    "            metrics = self.evaluate_dataset(predictions, ground_truths, questions)\n",
    "            \n",
    "            system_data = {'System': system_name}\n",
    "            system_data.update(metrics)\n",
    "            comparison_data.append(system_data)\n",
    "        \n",
    "        return pd.DataFrame(comparison_data)\n",
    "    \n",
    "    def detailed_error_analysis(self, predictions: List[str], ground_truths: List[str], \n",
    "                               questions: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Perform detailed error analysis\"\"\"\n",
    "        error_types = {\n",
    "            'exact_match_failures': [],\n",
    "            'low_semantic_similarity': [],\n",
    "            'irrelevant_answers': [],\n",
    "            'empty_predictions': []\n",
    "        }\n",
    "        \n",
    "        for i, (pred, truth, question) in enumerate(zip(predictions, ground_truths, questions)):\n",
    "            metrics = self.evaluate_single(pred, truth, question)\n",
    "            \n",
    "            if metrics['exact_match'] == 0:\n",
    "                error_types['exact_match_failures'].append({\n",
    "                    'index': i,\n",
    "                    'question': question,\n",
    "                    'predicted': pred,\n",
    "                    'ground_truth': truth,\n",
    "                    'token_f1': metrics['token_f1']\n",
    "                })\n",
    "            \n",
    "            if metrics['semantic_similarity'] < 0.3:\n",
    "                error_types['low_semantic_similarity'].append({\n",
    "                    'index': i,\n",
    "                    'question': question,\n",
    "                    'predicted': pred,\n",
    "                    'ground_truth': truth,\n",
    "                    'similarity': metrics['semantic_similarity']\n",
    "                })\n",
    "            \n",
    "            if metrics['answer_relevance'] < 0.2:\n",
    "                error_types['irrelevant_answers'].append({\n",
    "                    'index': i,\n",
    "                    'question': question,\n",
    "                    'predicted': pred,\n",
    "                    'relevance': metrics['answer_relevance']\n",
    "                })\n",
    "            \n",
    "            if not pred.strip():\n",
    "                error_types['empty_predictions'].append({\n",
    "                    'index': i,\n",
    "                    'question': question,\n",
    "                    'ground_truth': truth\n",
    "                })\n",
    "        \n",
    "        return error_types\n",
    "\n",
    "# Create evaluation dataset\n",
    "eval_questions = [\n",
    "    \"Who was Albert Einstein?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"When was the Eiffel Tower completed?\",\n",
    "    \"How tall is Mount Everest?\",\n",
    "    \"Who painted the Mona Lisa?\"\n",
    "]\n",
    "\n",
    "ground_truth_answers = [\n",
    "    \"Albert Einstein was a German-born theoretical physicist\",\n",
    "    \"Paris\",\n",
    "    \"1889\",\n",
    "    \"8,848.86 meters\",\n",
    "    \"Leonardo da Vinci\"\n",
    "]\n",
    "\n",
    "# Get predictions from different systems\n",
    "rule_predictions = []\n",
    "retrieval_predictions = []\n",
    "rc_predictions = []\n",
    "\n",
    "# Rule-based predictions\n",
    "for question in eval_questions:\n",
    "    result = rule_qa.answer_question(question)\n",
    "    rule_predictions.append(result['answer'])\n",
    "\n",
    "# Retrieval-based predictions\n",
    "for question in eval_questions:\n",
    "    result = retrieval_qa.answer_question(question)\n",
    "    retrieval_predictions.append(result['answer'])\n",
    "\n",
    "# Reading comprehension predictions (using Einstein passage for all questions)\n",
    "for question in eval_questions:\n",
    "    result = rc_qa.answer_question(test_passage, question)\n",
    "    rc_predictions.append(result['answer'])\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = QAEvaluator()\n",
    "\n",
    "# Evaluate systems\n",
    "system_results = {\n",
    "    'Rule-based': rule_predictions,\n",
    "    'Retrieval-based': retrieval_predictions,\n",
    "    'Reading Comprehension': rc_predictions\n",
    "}\n",
    "\n",
    "print(\"\\nQA System Evaluation Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_df = evaluator.compare_systems(system_results, ground_truth_answers, eval_questions)\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Detailed evaluation for retrieval-based system\n",
    "print(\"\\nDetailed Evaluation for Retrieval-based System:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, (question, pred, truth) in enumerate(zip(eval_questions, retrieval_predictions, ground_truth_answers)):\n",
    "    metrics = evaluator.evaluate_single(pred, truth, question)\n",
    "    print(f\"\\nQuestion {i+1}: {question}\")\n",
    "    print(f\"Predicted: {pred}\")\n",
    "    print(f\"Ground Truth: {truth}\")\n",
    "    print(f\"Exact Match: {metrics['exact_match']:.3f}\")\n",
    "    print(f\"Token F1: {metrics['token_f1']:.3f}\")\n",
    "    print(f\"Semantic Similarity: {metrics['semantic_similarity']:.3f}\")\n",
    "    print(f\"Answer Relevance: {metrics['answer_relevance']:.3f}\")\n",
    "\n",
    "# Error analysis\n",
    "print(\"\\nError Analysis for Retrieval-based System:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "error_analysis = evaluator.detailed_error_analysis(retrieval_predictions, ground_truth_answers, eval_questions)\n",
    "\n",
    "for error_type, errors in error_analysis.items():\n",
    "    if errors:\n",
    "        print(f\"\\n{error_type.replace('_', ' ').title()}: {len(errors)} cases\")\n",
    "        for error in errors[:2]:  # Show first 2 examples\n",
    "            print(f\"  Q: {error['question']}\")\n",
    "            print(f\"  Predicted: {error.get('predicted', 'N/A')}\")\n",
    "            if 'ground_truth' in error:\n",
    "                print(f\"  Expected: {error['ground_truth']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Question Answering Challenges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Challenge 1: Enhanced Question Classification\n",
    "Improve the question classification system to handle more complex question types and patterns.\n",
    "\n",
    "**Requirements:**\n",
    "- Add support for complex question types (why, how much, which one, etc.)\n",
    "- Implement question complexity scoring\n",
    "- Handle multi-part questions\n",
    "- Add confidence scoring for question classification\n",
    "\n",
    "**Success Criteria:**\n",
    "- Support 15+ question types\n",
    "- Achieve 90%+ accuracy on question classification\n",
    "- Handle compound questions correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 1\n",
    "class EnhancedQuestionClassifier:\n",
    "    def __init__(self):\n",
    "        # TODO: Define comprehensive question patterns\n",
    "        self.question_patterns = {}\n",
    "        self.complexity_indicators = []\n",
    "    \n",
    "    def classify_question_type(self, question):\n",
    "        # TODO: Implement enhanced question classification\n",
    "        pass\n",
    "    \n",
    "    def calculate_complexity_score(self, question):\n",
    "        # TODO: Calculate question complexity\n",
    "        pass\n",
    "    \n",
    "    def handle_compound_questions(self, question):\n",
    "        # TODO: Split and handle compound questions\n",
    "        pass\n",
    "    \n",
    "    def get_classification_confidence(self, question):\n",
    "        # TODO: Calculate confidence score\n",
    "        pass\n",
    "\n",
    "# Test your enhanced classifier\n",
    "test_questions = [\n",
    "    \"Who was Einstein and what did he discover?\",\n",
    "    \"Why is the sky blue?\",\n",
    "    \"How much does an elephant weigh?\",\n",
    "    \"Which planet is closest to the sun?\"\n",
    "]\n",
    "\n",
    "# TODO: Test your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Knowledge Graph QA\n",
    "Build a question answering system that uses a knowledge graph to answer questions.\n",
    "\n",
    "**Requirements:**\n",
    "- Create a simple knowledge graph structure\n",
    "- Implement graph traversal for answer finding\n",
    "- Handle relationship-based queries\n",
    "- Support multi-hop reasoning\n",
    "\n",
    "**Success Criteria:**\n",
    "- Build knowledge graph with 100+ entities\n",
    "- Support relationship queries (\"Who is married to X?\")\n",
    "- Handle 2-hop reasoning questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 2\n",
    "import networkx as nx\n",
    "\n",
    "class KnowledgeGraphQA:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize knowledge graph\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.entities = {}\n",
    "        self.relations = {}\n",
    "    \n",
    "    def build_knowledge_graph(self, facts):\n",
    "        # TODO: Build graph from facts\n",
    "        pass\n",
    "    \n",
    "    def parse_query(self, question):\n",
    "        # TODO: Parse question into graph query\n",
    "        pass\n",
    "    \n",
    "    def traverse_graph(self, start_entity, relation, hops=1):\n",
    "        # TODO: Traverse graph to find answers\n",
    "        pass\n",
    "    \n",
    "    def answer_relationship_query(self, question):\n",
    "        # TODO: Answer relationship-based questions\n",
    "        pass\n",
    "\n",
    "# Create sample knowledge graph\n",
    "kg_facts = [\n",
    "    (\"Einstein\", \"born_in\", \"Germany\"),\n",
    "    (\"Einstein\", \"developed\", \"Relativity Theory\"),\n",
    "    (\"Paris\", \"capital_of\", \"France\"),\n",
    "    (\"Eiffel Tower\", \"located_in\", \"Paris\")\n",
    "]\n",
    "\n",
    "# TODO: Test your knowledge graph QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Challenge 3: Multi-Document QA\n",
    "Build a system that can answer questions by combining information from multiple documents.\n",
    "\n",
    "**Requirements:**\n",
    "- Retrieve relevant passages from multiple documents\n",
    "- Combine information across documents\n",
    "- Handle conflicting information\n",
    "- Provide source attribution\n",
    "\n",
    "**Success Criteria:**\n",
    "- Work with 100+ documents\n",
    "- Successfully combine information from 3+ sources\n",
    "- Handle contradictory information appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 3\n",
    "class MultiDocumentQA:\n",
    "    def __init__(self, document_collection):\n",
    "        # TODO: Initialize multi-document QA system\n",
    "        self.documents = document_collection\n",
    "        self.passage_retriever = None\n",
    "        self.information_aggregator = None\n",
    "    \n",
    "    def retrieve_relevant_passages(self, question, top_k=10):\n",
    "        # TODO: Retrieve passages from multiple documents\n",
    "        pass\n",
    "    \n",
    "    def detect_information_conflicts(self, passages):\n",
    "        # TODO: Detect conflicting information\n",
    "        pass\n",
    "    \n",
    "    def aggregate_information(self, passages, question):\n",
    "        # TODO: Combine information from multiple sources\n",
    "        pass\n",
    "    \n",
    "    def provide_source_attribution(self, answer, sources):\n",
    "        # TODO: Attribute answer to sources\n",
    "        pass\n",
    "    \n",
    "    def answer_multi_document_question(self, question):\n",
    "        # TODO: Main method for multi-document QA\n",
    "        pass\n",
    "\n",
    "# TODO: Test with multiple documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: Visual Question Answering\n",
    "Extend the QA system to answer questions about images.\n",
    "\n",
    "**Requirements:**\n",
    "- Extract visual features from images\n",
    "- Combine visual and textual information\n",
    "- Handle spatial reasoning questions\n",
    "- Support counting and color recognition\n",
    "\n",
    "**Success Criteria:**\n",
    "- Answer questions about objects in images\n",
    "- Handle spatial relationships (\"What is to the left of X?\")\n",
    "- Count objects accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 4\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class VisualQA:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize visual QA system\n",
    "        self.feature_extractor = None\n",
    "        self.object_detector = None\n",
    "        self.spatial_reasoner = None\n",
    "    \n",
    "    def extract_visual_features(self, image_path):\n",
    "        # TODO: Extract features from image\n",
    "        pass\n",
    "    \n",
    "    def detect_objects(self, image):\n",
    "        # TODO: Detect objects in image\n",
    "        pass\n",
    "    \n",
    "    def analyze_spatial_relationships(self, objects):\n",
    "        # TODO: Analyze spatial relationships\n",
    "        pass\n",
    "    \n",
    "    def count_objects(self, image, object_type):\n",
    "        # TODO: Count specific objects\n",
    "        pass\n",
    "    \n",
    "    def answer_visual_question(self, image_path, question):\n",
    "        # TODO: Answer questions about images\n",
    "        pass\n",
    "\n",
    "# TODO: Test with sample images and questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Challenge 5: Reasoning-Intensive QA\n",
    "Build a QA system that can handle complex reasoning tasks requiring multiple inference steps.\n",
    "\n",
    "**Requirements:**\n",
    "- Implement logical reasoning capabilities\n",
    "- Handle mathematical word problems\n",
    "- Support causal reasoning\n",
    "- Provide step-by-step explanations\n",
    "\n",
    "**Success Criteria:**\n",
    "- Solve multi-step mathematical problems\n",
    "- Handle logical puzzles and inference chains\n",
    "- Provide clear reasoning explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 5\n",
    "class ReasoningQA:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize reasoning QA system\n",
    "        self.logical_reasoner = None\n",
    "        self.math_solver = None\n",
    "        self.causal_reasoner = None\n",
    "    \n",
    "    def parse_mathematical_problem(self, question):\n",
    "        # TODO: Parse math word problems\n",
    "        pass\n",
    "    \n",
    "    def solve_mathematical_problem(self, problem):\n",
    "        # TODO: Solve mathematical problems step by step\n",
    "        pass\n",
    "    \n",
    "    def perform_logical_inference(self, premises, conclusion):\n",
    "        # TODO: Perform logical reasoning\n",
    "        pass\n",
    "    \n",
    "    def analyze_causal_relationships(self, events):\n",
    "        # TODO: Analyze cause and effect\n",
    "        pass\n",
    "    \n",
    "    def generate_explanation(self, reasoning_steps):\n",
    "        # TODO: Generate step-by-step explanations\n",
    "        pass\n",
    "    \n",
    "    def answer_reasoning_question(self, question):\n",
    "        # TODO: Main method for reasoning-intensive QA\n",
    "        pass\n",
    "\n",
    "# Test reasoning questions\n",
    "reasoning_questions = [\n",
    "    \"If John has 5 apples and gives 2 to Mary, how many does he have left?\",\n",
    "    \"All birds can fly. Penguins are birds. Can penguins fly?\",\n",
    "    \"If it rains, the ground gets wet. The ground is wet. Did it rain?\"\n",
    "]\n",
    "\n",
    "# TODO: Test your reasoning QA system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 6: Real-time Collaborative QA\n",
    "Build a real-time QA system that can learn from user feedback and collaborate with human experts.\n",
    "\n",
    "**Requirements:**\n",
    "- Implement online learning from user feedback\n",
    "- Route difficult questions to human experts\n",
    "- Learn from expert annotations\n",
    "- Maintain quality over time\n",
    "\n",
    "**Success Criteria:**\n",
    "- Improve accuracy through user feedback\n",
    "- Efficiently route questions to appropriate experts\n",
    "- Demonstrate continuous learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 6\n",
    "import asyncio\n",
    "from queue import Queue\n",
    "import threading\n",
    "\n",
    "class CollaborativeQA:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize collaborative QA system\n",
    "        self.base_qa_system = None\n",
    "        self.expert_queue = Queue()\n",
    "        self.feedback_learner = None\n",
    "        self.confidence_threshold = 0.8\n",
    "    \n",
    "    def assess_question_difficulty(self, question):\n",
    "        # TODO: Assess if question needs expert help\n",
    "        pass\n",
    "    \n",
    "    def route_to_expert(self, question, domain):\n",
    "        # TODO: Route question to appropriate expert\n",
    "        pass\n",
    "    \n",
    "    def collect_user_feedback(self, question, answer, feedback):\n",
    "        # TODO: Collect and process user feedback\n",
    "        pass\n",
    "    \n",
    "    def update_model_from_feedback(self, feedback_data):\n",
    "        # TODO: Update model based on feedback\n",
    "        pass\n",
    "    \n",
    "    def learn_from_expert_annotations(self, annotations):\n",
    "        # TODO: Learn from expert corrections\n",
    "        pass\n",
    "    \n",
    "    def answer_with_collaboration(self, question):\n",
    "        # TODO: Main collaborative answering method\n",
    "        pass\n",
    "\n",
    "# TODO: Implement collaborative QA system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Bonus Challenge: Production QA System**\n",
    "\n",
    "Build a complete, production-ready question answering system with enterprise features.\n",
    "\n",
    "### Requirements:\n",
    "1. **Multi-modal Support**: Text, images, and audio questions\n",
    "2. **Real-time Performance**: < 200ms response time\n",
    "3. **Scalable Architecture**: Handle 100K+ questions per day\n",
    "4. **Quality Assurance**: Automated quality checks\n",
    "5. **Analytics Dashboard**: Usage and performance metrics\n",
    "6. **API Management**: Rate limiting, authentication\n",
    "7. **Model Management**: A/B testing, versioning\n",
    "8. **Monitoring**: Real-time alerts and logging\n",
    "9. **Multi-language Support**: 5+ languages\n",
    "10. **Security**: Data privacy and access control\n",
    "\n",
    "### Success Criteria:\n",
    "- Handle 1000+ concurrent users\n",
    "- 99.9% uptime\n",
    "- Comprehensive API documentation\n",
    "- Automated testing pipeline\n",
    "- Docker and Kubernetes deployment\n",
    "- Real-time monitoring dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Bonus Challenge\n",
    "from flask import Flask, request, jsonify\n",
    "import redis\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "\n",
    "class ProductionQASystem:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize production QA system\n",
    "        self.app = Flask(__name__)\n",
    "        self.qa_models = {}  # Model registry\n",
    "        self.cache = redis.Redis()\n",
    "        self.analytics = {}\n",
    "        self.setup_logging()\n",
    "        self.setup_routes()\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        # TODO: Set up comprehensive logging\n",
    "        pass\n",
    "    \n",
    "    def setup_routes(self):\n",
    "        # TODO: Define API endpoints\n",
    "        @self.app.route('/ask', methods=['POST'])\n",
    "        def ask_question():\n",
    "            # TODO: Main QA endpoint\n",
    "            pass\n",
    "        \n",
    "        @self.app.route('/batch_ask', methods=['POST'])\n",
    "        def batch_questions():\n",
    "            # TODO: Batch processing endpoint\n",
    "            pass\n",
    "        \n",
    "        @self.app.route('/analytics', methods=['GET'])\n",
    "        def get_analytics():\n",
    "            # TODO: Analytics dashboard\n",
    "            pass\n",
    "    \n",
    "    def authenticate_request(self, request):\n",
    "        # TODO: Implement authentication\n",
    "        pass\n",
    "    \n",
    "    def rate_limit_check(self, user_id):\n",
    "        # TODO: Implement rate limiting\n",
    "        pass\n",
    "    \n",
    "    def quality_check(self, question, answer):\n",
    "        # TODO: Automated quality checks\n",
    "        pass\n",
    "    \n",
    "    def log_interaction(self, question, answer, response_time):\n",
    "        # TODO: Log all interactions\n",
    "        pass\n",
    "    \n",
    "    def monitor_performance(self):\n",
    "        # TODO: Real-time performance monitoring\n",
    "        pass\n",
    "\n",
    "# Docker configuration\n",
    "dockerfile = \"\"\"\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 5000\n",
    "\n",
    "CMD [\"gunicorn\", \"--bind\", \"0.0.0.0:5000\", \"app:app\"]\n",
    "\"\"\"\n",
    "\n",
    "# Kubernetes deployment\n",
    "k8s_config = \"\"\"\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: qa-system\n",
    "spec:\n",
    "  replicas: 5\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: qa-system\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: qa-system\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: qa-system\n",
    "        image: qa-system:latest\n",
    "        ports:\n",
    "        - containerPort: 5000\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "          limits:\n",
    "            memory: \"2Gi\"\n",
    "            cpu: \"1000m\"\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Complete the production system implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}