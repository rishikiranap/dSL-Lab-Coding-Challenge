{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dslmllab/dSL-Lab-Coding-Challenge/blob/main/3_named_entity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwaUJe4KzOGh"
      },
      "source": [
        "# Named Entity Recognition (NER)\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "At the end of this notebook, you will be able to:\n",
        "\n",
        "1. Understand the fundamentals of Named Entity Recognition\n",
        "2. Implement rule-based NER approaches\n",
        "3. Build machine learning models for NER\n",
        "4. Use pre-trained models for entity extraction\n",
        "5. Evaluate NER system performance\n",
        "6. Handle multi-class entity classification\n",
        "7. Build custom entity recognition systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVbp3QNgzOGj"
      },
      "source": [
        "## Introduction to Named Entity Recognition\n",
        "\n",
        "Named Entity Recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as:\n",
        "\n",
        "- **PERSON**: Names of people\n",
        "- **ORGANIZATION**: Companies, agencies, institutions\n",
        "- **LOCATION**: Countries, cities, addresses\n",
        "- **DATE**: Absolute or relative dates or periods\n",
        "- **TIME**: Times smaller than a day\n",
        "- **MONEY**: Monetary values\n",
        "- **PERCENT**: Percentage values\n",
        "- **FACILITY**: Buildings, airports, highways, bridges\n",
        "\n",
        "### Why is NER Important?\n",
        "\n",
        "1. **Information Extraction**: Extract structured information from unstructured text\n",
        "2. **Search Enhancement**: Improve search results by understanding entity types\n",
        "3. **Knowledge Graphs**: Build relationships between entities\n",
        "4. **Question Answering**: Identify entities relevant to questions\n",
        "5. **Content Classification**: Categorize documents based on entities\n",
        "\n",
        "### Approaches to NER\n",
        "\n",
        "1. **Rule-based**: Using patterns, dictionaries, and linguistic rules\n",
        "2. **Statistical**: Using machine learning models (CRF, SVM)\n",
        "3. **Deep Learning**: Using neural networks (BiLSTM-CRF, BERT)\n",
        "4. **Hybrid**: Combining multiple approaches"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install numpy pandas matplotlib seaborn nltk spacy scikit-learn sklearn-crfsuite tqdm\n"
      ],
      "metadata": {
        "id": "WRZynIPlzUMa",
        "outputId": "df718107-47b1-4660-de60-de3bda47debf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.7 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite) (0.9.11)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite) (0.9.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onWE6IR0zOGj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Download spaCy English model if not present\n",
        "\n",
        "import spacy\n",
        "\n",
        "try:\n",
        "    spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    !python -m spacy download en_core_web_sm\n",
        "\n",
        "# Download required NLTK data\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk_downloads = ['punkt', 'stopwords', 'wordnet','maxent_ne_chunker_tab', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words','punkt_tab','averaged_perceptron_tagger_eng']\n",
        "for item in nltk_downloads:\n",
        "    nltk.download(item, quiet=True)\n",
        "\n",
        "#for item in ['punkt', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words', 'conll2002']:\n",
        "    #nltk.download(item, quiet=True)"
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Kuq7p7-xzOGk",
        "outputId": "976ca068-6a99-4592-e720-b99c89cfebd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn_crfsuite import CRF\n",
        "from sklearn_crfsuite.metrics import flat_classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('maxent_ne_chunker', quiet=True)\n",
        "nltk.download('words', quiet=True)\n",
        "nltk.download('conll2002', quiet=True)\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNRhFcBXzOGk"
      },
      "source": [
        "## 1. Rule-Based Named Entity Recognition\n",
        "\n",
        "Rule-based NER uses patterns, regular expressions, and dictionaries to identify entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YKQgK113zOGk",
        "outputId": "d4ba1bc5-0d71-43cb-fc9d-785862aafb13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "\n",
            "Dr. Smith from Google will meet with Ms. Johnson at 3:30 PM on 12/25/2023 in New York.\n",
            "Please contact him at john.smith@gmail.com or call (555) 123-4567.\n",
            "The project budget is $50,000 with a 15% contingency.\n",
            "Visit our website at https://www.example.com for more details.\n",
            "\n",
            "\n",
            "Extracted entities:\n",
            "EMAIL: ['john.smith@gmail.com']\n",
            "DATE: ['12/25/2023']\n",
            "TIME: ['3:30 PM']\n",
            "MONEY: ['$50,000']\n",
            "URL: ['https://www.example.com']\n",
            "ORGANIZATION: ['google']\n",
            "\n",
            "Annotated text:\n",
            "\n",
            "Dr. Smith from Google will meet with Ms. Johnson at [3:30 PM]_TIME on [12/25/2023]_DATE in New York.\n",
            "Please contact him at [john.smith@gmail.com]_EMAIL or call (555) 123-4567.\n",
            "The project budget is [$50,000]_MONEY with a 15% contingency.\n",
            "Visit our website at [https://www.example.com]_URL for more details.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class RuleBasedNER:\n",
        "    def __init__(self):\n",
        "        # Pattern definitions\n",
        "        self.patterns = {\n",
        "            'EMAIL': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
        "            'PHONE': r'\\b(?:\\+?1[-.]?)?\\(?[0-9]{3}\\)?[-.]?[0-9]{3}[-.]?[0-9]{4}\\b',\n",
        "            'DATE': r'\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}[/-]\\d{1,2}[/-]\\d{1,2})\\b',\n",
        "            'TIME': r'\\b(?:[01]?\\d|2[0-3]):[0-5]\\d(?:\\s?[AaPp][Mm])?\\b',\n",
        "            'MONEY': r'\\$\\s?\\d+(?:,\\d{3})*(?:\\.\\d{2})?\\b',\n",
        "            'PERCENT': r'\\b\\d+(?:\\.\\d+)?%\\b',\n",
        "            'URL': r'https?://(?:[-\\w.])+(?:[:\\d]+)?(?:/(?:[\\w/_.])*(?:\\?(?:[\\w&=%.])*)?(?:#(?:[\\w.])*)?)?'\n",
        "        }\n",
        "\n",
        "        # Entity dictionaries\n",
        "        self.person_titles = {'mr', 'mrs', 'ms', 'dr', 'prof', 'sir', 'madam'}\n",
        "        self.organizations = {'google', 'microsoft', 'apple', 'amazon', 'facebook', 'netflix'}\n",
        "        self.locations = {'new york', 'london', 'paris', 'tokyo', 'beijing', 'delhi'}\n",
        "\n",
        "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
        "        entities = {}\n",
        "\n",
        "        # Pattern-based extraction\n",
        "        for entity_type, pattern in self.patterns.items():\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            if matches:\n",
        "                entities[entity_type] = matches\n",
        "\n",
        "        # Dictionary-based extraction\n",
        "        words = text.lower().split()\n",
        "\n",
        "        # Person detection (simple heuristic)\n",
        "        persons = []\n",
        "        for i, word in enumerate(words):\n",
        "            if word in self.person_titles and i + 1 < len(words):\n",
        "                persons.append(f\"{word} {words[i+1]}\")\n",
        "        if persons:\n",
        "            entities['PERSON'] = persons\n",
        "\n",
        "        # Organization detection\n",
        "        orgs = [word for word in words if word in self.organizations]\n",
        "        if orgs:\n",
        "            entities['ORGANIZATION'] = orgs\n",
        "\n",
        "        # Location detection\n",
        "        locs = [word for word in words if word in self.locations]\n",
        "        if locs:\n",
        "            entities['LOCATION'] = locs\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def annotate_text(self, text: str) -> str:\n",
        "        \"\"\"Annotate text with entity tags\"\"\"\n",
        "        annotated = text\n",
        "        entities = self.extract_entities(text)\n",
        "\n",
        "        for entity_type, entity_list in entities.items():\n",
        "            for entity in entity_list:\n",
        "                annotated = annotated.replace(entity, f\"[{entity}]_{entity_type}\")\n",
        "\n",
        "        return annotated\n",
        "\n",
        "# Test the rule-based NER\n",
        "ner = RuleBasedNER()\n",
        "\n",
        "sample_text = \"\"\"\n",
        "Dr. Smith from Google will meet with Ms. Johnson at 3:30 PM on 12/25/2023 in New York.\n",
        "Please contact him at john.smith@gmail.com or call (555) 123-4567.\n",
        "The project budget is $50,000 with a 15% contingency.\n",
        "Visit our website at https://www.example.com for more details.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Original text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nExtracted entities:\")\n",
        "entities = ner.extract_entities(sample_text)\n",
        "for ent_type, ent_list in entities.items():\n",
        "    print(f\"{ent_type}: {ent_list}\")\n",
        "\n",
        "print(\"\\nAnnotated text:\")\n",
        "print(ner.annotate_text(sample_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aez7LyK6zOGl"
      },
      "source": [
        "## 2. Feature-Based NER with Machine Learning\n",
        "\n",
        "We'll build features for each word and use machine learning to classify entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JeV4NILTzOGl",
        "outputId": "94dff99d-dbea-47df-a7ae-d368e50e34ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing training data...\n",
            "Training on 30 examples with 6 labels...\n",
            "Training completed!\n",
            "\n",
            "Test sentence with predictions:\n",
            "Elon: B-PER\n",
            "Musk: O\n",
            "founded: O\n",
            "Tesla: O\n",
            "Motors: O\n"
          ]
        }
      ],
      "source": [
        "class FeatureBasedNER:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = DictVectorizer()\n",
        "        self.model = LogisticRegression(max_iter=1000)\n",
        "        self.label_to_id = {}\n",
        "        self.id_to_label = {}\n",
        "\n",
        "    def extract_features(self, tokens: List[str], pos_tags: List[str], index: int) -> Dict[str, Any]:\n",
        "        \"\"\"Extract features for word at given index\"\"\"\n",
        "        word = tokens[index]\n",
        "        pos = pos_tags[index]\n",
        "\n",
        "        features = {\n",
        "            # Word features\n",
        "            'word': word.lower(),\n",
        "            'word_length': len(word),\n",
        "            'is_capitalized': word[0].isupper(),\n",
        "            'is_all_caps': word.isupper(),\n",
        "            'is_title_case': word.istitle(),\n",
        "            'is_numeric': word.isdigit(),\n",
        "            'has_digit': any(c.isdigit() for c in word),\n",
        "            'has_hyphen': '-' in word,\n",
        "            'has_dot': '.' in word,\n",
        "\n",
        "            # POS features\n",
        "            'pos': pos,\n",
        "            'is_noun': pos.startswith('N'),\n",
        "            'is_proper_noun': pos == 'NNP',\n",
        "\n",
        "            # Shape features\n",
        "            'word_shape': self.get_word_shape(word),\n",
        "\n",
        "            # Prefix/Suffix features\n",
        "            'prefix_2': word[:2].lower() if len(word) >= 2 else '',\n",
        "            'prefix_3': word[:3].lower() if len(word) >= 3 else '',\n",
        "            'suffix_2': word[-2:].lower() if len(word) >= 2 else '',\n",
        "            'suffix_3': word[-3:].lower() if len(word) >= 3 else '',\n",
        "        }\n",
        "\n",
        "        # Context features\n",
        "        if index > 0:\n",
        "            features['prev_word'] = tokens[index-1].lower()\n",
        "            features['prev_pos'] = pos_tags[index-1]\n",
        "        else:\n",
        "            features['prev_word'] = 'BOS'\n",
        "            features['prev_pos'] = 'BOS'\n",
        "\n",
        "        if index < len(tokens) - 1:\n",
        "            features['next_word'] = tokens[index+1].lower()\n",
        "            features['next_pos'] = pos_tags[index+1]\n",
        "        else:\n",
        "            features['next_word'] = 'EOS'\n",
        "            features['next_pos'] = 'EOS'\n",
        "\n",
        "        return features\n",
        "\n",
        "    def get_word_shape(self, word: str) -> str:\n",
        "        \"\"\"Get word shape (X=uppercase, x=lowercase, d=digit, p=punctuation)\"\"\"\n",
        "        shape = ''\n",
        "        for char in word:\n",
        "            if char.isupper():\n",
        "                shape += 'X'\n",
        "            elif char.islower():\n",
        "                shape += 'x'\n",
        "            elif char.isdigit():\n",
        "                shape += 'd'\n",
        "            else:\n",
        "                shape += 'p'\n",
        "        return shape\n",
        "\n",
        "    def prepare_data(self, sentences: List[List[Tuple[str, str]]]) -> Tuple[List[Dict], List[str]]:\n",
        "        \"\"\"Prepare features and labels from annotated sentences\"\"\"\n",
        "        features = []\n",
        "        labels = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            tokens = [token for token, _ in sentence]\n",
        "            tags = [tag for _, tag in sentence]\n",
        "\n",
        "            # Get POS tags\n",
        "            pos_tags = [pos for _, pos in nltk.pos_tag(tokens)]\n",
        "\n",
        "            for i in range(len(tokens)):\n",
        "                feat = self.extract_features(tokens, pos_tags, i)\n",
        "                features.append(feat)\n",
        "                labels.append(tags[i])\n",
        "\n",
        "        return features, labels\n",
        "\n",
        "    def train(self, train_sentences: List[List[Tuple[str, str]]]):\n",
        "        \"\"\"Train the NER model\"\"\"\n",
        "        print(\"Preparing training data...\")\n",
        "        features, labels = self.prepare_data(train_sentences)\n",
        "\n",
        "        # Create label mappings\n",
        "        unique_labels = list(set(labels))\n",
        "        self.label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
        "        self.id_to_label = {i: label for label, i in self.label_to_id.items()}\n",
        "\n",
        "        # Vectorize features and encode labels\n",
        "        X = self.vectorizer.fit_transform(features)\n",
        "        y = [self.label_to_id[label] for label in labels]\n",
        "\n",
        "        print(f\"Training on {len(features)} examples with {len(unique_labels)} labels...\")\n",
        "        self.model.fit(X, y)\n",
        "        print(\"Training completed!\")\n",
        "\n",
        "    def predict(self, sentence: List[str]) -> List[str]:\n",
        "        \"\"\"Predict entity labels for a sentence\"\"\"\n",
        "        pos_tags = [pos for _, pos in nltk.pos_tag(sentence)]\n",
        "        features = []\n",
        "\n",
        "        for i in range(len(sentence)):\n",
        "            feat = self.extract_features(sentence, pos_tags, i)\n",
        "            features.append(feat)\n",
        "\n",
        "        X = self.vectorizer.transform(features)\n",
        "        predictions = self.model.predict(X)\n",
        "\n",
        "        return [self.id_to_label[pred] for pred in predictions]\n",
        "\n",
        "# Create sample training data (in real scenarios, you'd use CoNLL format data)\n",
        "sample_training_data = [\n",
        "    [('John', 'B-PER'), ('Smith', 'I-PER'), ('works', 'O'), ('at', 'O'), ('Google', 'B-ORG'), ('in', 'O'), ('California', 'B-LOC')],\n",
        "    [('Apple', 'B-ORG'), ('Inc', 'I-ORG'), ('is', 'O'), ('located', 'O'), ('in', 'O'), ('Cupertino', 'B-LOC')],\n",
        "    [('Barack', 'B-PER'), ('Obama', 'I-PER'), ('was', 'O'), ('born', 'O'), ('in', 'O'), ('Hawaii', 'B-LOC')],\n",
        "    [('Microsoft', 'B-ORG'), ('Corporation', 'I-ORG'), ('headquarters', 'O'), ('in', 'O'), ('Seattle', 'B-LOC')],\n",
        "    [('The', 'O'), ('meeting', 'O'), ('is', 'O'), ('scheduled', 'O'), ('for', 'O'), ('Monday', 'O')]\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "ml_ner = FeatureBasedNER()\n",
        "ml_ner.train(sample_training_data)\n",
        "\n",
        "# Test prediction\n",
        "test_sentence = ['Elon', 'Musk', 'founded', 'Tesla', 'Motors']\n",
        "predictions = ml_ner.predict(test_sentence)\n",
        "\n",
        "print(\"\\nTest sentence with predictions:\")\n",
        "for word, pred in zip(test_sentence, predictions):\n",
        "    print(f\"{word}: {pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJhubv7DzOGm"
      },
      "source": [
        "## 3. Conditional Random Fields (CRF) for NER\n",
        "\n",
        "CRF is particularly effective for sequence labeling tasks like NER."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uWsQSLrHzOGm",
        "outputId": "3438294c-86ab-4e1a-d913-ce7de8e909d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing CRF training data...\n",
            "Training CRF on 10 sentences...\n",
            "CRF training completed!\n",
            "\n",
            "CRF Model Predictions:\n",
            "Sentence: Mark Zuckerberg founded Facebook in California\n",
            "  Mark: B-PER\n",
            "  Zuckerberg: I-PER\n",
            "  founded: O\n",
            "  Facebook: B-ORG\n",
            "  in: O\n",
            "  California: B-LOC\n",
            "\n",
            "Sentence: IBM has offices in New York\n",
            "  IBM: B-ORG\n",
            "  has: O\n",
            "  offices: O\n",
            "  in: O\n",
            "  New: B-LOC\n",
            "  York: I-LOC\n",
            "\n",
            "Sentence: Toyota manufactures cars in Japan\n",
            "  Toyota: B-ORG\n",
            "  manufactures: O\n",
            "  cars: O\n",
            "  in: O\n",
            "  Japan: B-LOC\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class CRFBasedNER:\n",
        "    def __init__(self):\n",
        "        self.crf = CRF(\n",
        "            algorithm='lbfgs',\n",
        "            c1=0.1,\n",
        "            c2=0.1,\n",
        "            max_iterations=100,\n",
        "            all_possible_transitions=True\n",
        "        )\n",
        "\n",
        "    def word_features(self, sentence: List[str], i: int) -> Dict[str, Any]:\n",
        "        \"\"\"Extract features for word at position i in sentence\"\"\"\n",
        "        word = sentence[i]\n",
        "        pos_tags = [pos for _, pos in nltk.pos_tag(sentence)]\n",
        "\n",
        "        features = {\n",
        "            'bias': 1.0,\n",
        "            'word.lower()': word.lower(),\n",
        "            'word[-3:]': word[-3:],\n",
        "            'word[-2:]': word[-2:],\n",
        "            'word.isupper()': word.isupper(),\n",
        "            'word.istitle()': word.istitle(),\n",
        "            'word.isdigit()': word.isdigit(),\n",
        "            'postag': pos_tags[i],\n",
        "            'postag[:2]': pos_tags[i][:2],\n",
        "        }\n",
        "\n",
        "        if i > 0:\n",
        "            word1 = sentence[i-1]\n",
        "            postag1 = pos_tags[i-1]\n",
        "            features.update({\n",
        "                '-1:word.lower()': word1.lower(),\n",
        "                '-1:word.istitle()': word1.istitle(),\n",
        "                '-1:word.isupper()': word1.isupper(),\n",
        "                '-1:postag': postag1,\n",
        "                '-1:postag[:2]': postag1[:2],\n",
        "            })\n",
        "        else:\n",
        "            features['BOS'] = True\n",
        "\n",
        "        if i < len(sentence) - 1:\n",
        "            word1 = sentence[i+1]\n",
        "            postag1 = pos_tags[i+1]\n",
        "            features.update({\n",
        "                '+1:word.lower()': word1.lower(),\n",
        "                '+1:word.istitle()': word1.istitle(),\n",
        "                '+1:word.isupper()': word1.isupper(),\n",
        "                '+1:postag': postag1,\n",
        "                '+1:postag[:2]': postag1[:2],\n",
        "            })\n",
        "        else:\n",
        "            features['EOS'] = True\n",
        "\n",
        "        return features\n",
        "\n",
        "    def sentence_features(self, sentence: List[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Extract features for entire sentence\"\"\"\n",
        "        return [self.word_features(sentence, i) for i in range(len(sentence))]\n",
        "\n",
        "    def prepare_data(self, sentences: List[List[Tuple[str, str]]]) -> Tuple[List[List[Dict]], List[List[str]]]:\n",
        "        \"\"\"Prepare CRF training data\"\"\"\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            words = [word for word, _ in sentence]\n",
        "            labels = [label for _, label in sentence]\n",
        "\n",
        "            X.append(self.sentence_features(words))\n",
        "            y.append(labels)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def train(self, train_sentences: List[List[Tuple[str, str]]]):\n",
        "        \"\"\"Train CRF model\"\"\"\n",
        "        print(\"Preparing CRF training data...\")\n",
        "        X_train, y_train = self.prepare_data(train_sentences)\n",
        "\n",
        "        print(f\"Training CRF on {len(X_train)} sentences...\")\n",
        "        self.crf.fit(X_train, y_train)\n",
        "        print(\"CRF training completed!\")\n",
        "\n",
        "    def predict(self, sentence: List[str]) -> List[str]:\n",
        "        \"\"\"Predict labels for sentence\"\"\"\n",
        "        features = self.sentence_features(sentence)\n",
        "        return self.crf.predict([features])[0]\n",
        "\n",
        "    def evaluate(self, test_sentences: List[List[Tuple[str, str]]]) -> str:\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "        X_test, y_test = self.prepare_data(test_sentences)\n",
        "        y_pred = self.crf.predict(X_test)\n",
        "\n",
        "        return flat_classification_report(y_test, y_pred)\n",
        "\n",
        "# Extended training data for CRF\n",
        "extended_training_data = sample_training_data + [\n",
        "    [('Amazon', 'B-ORG'), ('Web', 'I-ORG'), ('Services', 'I-ORG'), ('hosts', 'O'), ('in', 'O'), ('Virginia', 'B-LOC')],\n",
        "    [('Netflix', 'B-ORG'), ('streams', 'O'), ('globally', 'O'), ('from', 'O'), ('Los', 'B-LOC'), ('Angeles', 'I-LOC')],\n",
        "    [('Tim', 'B-PER'), ('Cook', 'I-PER'), ('leads', 'O'), ('Apple', 'B-ORG'), ('today', 'O')],\n",
        "    [('Facebook', 'B-ORG'), ('changed', 'O'), ('to', 'O'), ('Meta', 'B-ORG'), ('recently', 'O')],\n",
        "    [('London', 'B-LOC'), ('is', 'O'), ('the', 'O'), ('capital', 'O'), ('of', 'O'), ('England', 'B-LOC')]\n",
        "]\n",
        "\n",
        "# Train CRF model\n",
        "crf_ner = CRFBasedNER()\n",
        "crf_ner.train(extended_training_data)\n",
        "\n",
        "# Test CRF model\n",
        "test_sentences_crf = [\n",
        "    ['Mark', 'Zuckerberg', 'founded', 'Facebook', 'in', 'California'],\n",
        "    ['IBM', 'has', 'offices', 'in', 'New', 'York'],\n",
        "    ['Toyota', 'manufactures', 'cars', 'in', 'Japan']\n",
        "]\n",
        "\n",
        "print(\"\\nCRF Model Predictions:\")\n",
        "for sentence in test_sentences_crf:\n",
        "    predictions = crf_ner.predict(sentence)\n",
        "    print(f\"Sentence: {' '.join(sentence)}\")\n",
        "    for word, pred in zip(sentence, predictions):\n",
        "        print(f\"  {word}: {pred}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmAboTuezOGm"
      },
      "source": [
        "## 4. Using Pre-trained NER Models\n",
        "\n",
        "We'll use spaCy's pre-trained models for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LLA6aiPDzOGm",
        "outputId": "65ce9204-110f-49e8-da9d-6cd98d747767",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Apple Inc. was founded by Steve Jobs in Cupertino, California on April 1, 1976.\n",
            "\n",
            "1. Rule-based NER:\n",
            "   ORGANIZATION: ['apple']\n",
            "\n",
            "2. NLTK NER:\n",
            "   PERSON: Apple\n",
            "   ORGANIZATION: Inc.\n",
            "   PERSON: Steve Jobs\n",
            "   GPE: Cupertino\n",
            "   GPE: California\n",
            "\n",
            "3. SpaCy NER:\n",
            "   ORG: Apple Inc.\n",
            "   PERSON: Steve Jobs\n",
            "   GPE: Cupertino\n",
            "   GPE: California\n",
            "   DATE: April 1, 1976\n",
            "\n",
            "============================================================\n",
            "Text: Elon Musk, CEO of Tesla and SpaceX, was born in South Africa and now lives in Texas.\n",
            "\n",
            "1. Rule-based NER:\n",
            "\n",
            "2. NLTK NER:\n",
            "   PERSON: Elon\n",
            "   GPE: Musk\n",
            "   ORGANIZATION: CEO\n",
            "   GPE: Tesla\n",
            "   ORGANIZATION: SpaceX\n",
            "   GPE: South Africa\n",
            "   GPE: Texas\n",
            "\n",
            "3. SpaCy NER:\n",
            "   PERSON: Elon Musk\n",
            "   ORG: Tesla\n",
            "   PERSON: SpaceX\n",
            "   GPE: South Africa\n",
            "   GPE: Texas\n"
          ]
        }
      ],
      "source": [
        "# Note: This requires spaCy model installation\n",
        "# Run: python -m spacy download en_core_web_sm\n",
        "\n",
        "def compare_ner_models(text: str):\n",
        "    \"\"\"Compare different NER approaches on the same text\"\"\"\n",
        "    print(f\"Text: {text}\\n\")\n",
        "\n",
        "    # Rule-based NER\n",
        "    print(\"1. Rule-based NER:\")\n",
        "    rule_entities = ner.extract_entities(text)\n",
        "    for ent_type, entities in rule_entities.items():\n",
        "        print(f\"   {ent_type}: {entities}\")\n",
        "\n",
        "    # NLTK NER\n",
        "    print(\"\\n2. NLTK NER:\")\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    chunks = nltk.ne_chunk(pos_tags)\n",
        "\n",
        "    nltk_entities = []\n",
        "    for chunk in chunks:\n",
        "        if hasattr(chunk, 'label'):\n",
        "            entity_name = ' '.join([token for token, pos in chunk.leaves()])\n",
        "            entity_type = chunk.label()\n",
        "            nltk_entities.append((entity_name, entity_type))\n",
        "\n",
        "    for entity_name, entity_type in nltk_entities:\n",
        "        print(f\"   {entity_type}: {entity_name}\")\n",
        "\n",
        "    # SpaCy NER (if available)\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        doc = nlp(text)\n",
        "        print(\"\\n3. SpaCy NER:\")\n",
        "        for ent in doc.ents:\n",
        "            print(f\"   {ent.label_}: {ent.text}\")\n",
        "    except OSError:\n",
        "        print(\"\\n3. SpaCy NER: (Model not installed)\")\n",
        "        print(\"   Run: python -m spacy download en_core_web_sm\")\n",
        "\n",
        "# Test comparison\n",
        "comparison_text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California on April 1, 1976.\"\n",
        "compare_ner_models(comparison_text)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "comparison_text2 = \"Elon Musk, CEO of Tesla and SpaceX, was born in South Africa and now lives in Texas.\"\n",
        "compare_ner_models(comparison_text2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJx6SylvzOGn"
      },
      "source": [
        "## 5. NER Evaluation Metrics\n",
        "\n",
        "Understanding how to evaluate NER systems properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OA6G8cOlzOGn",
        "outputId": "0b4aeaf6-2232-4f37-fcb2-efe92324506e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER Evaluation Report:\n",
            "Entity Type     Precision  Recall     F1-Score   Support   \n",
            "-----------------------------------------------------------------\n",
            "LOC             0.333      0.500      0.400      2         \n",
            "PER             1.000      1.000      1.000      2         \n",
            "ORG             0.667      0.667      0.667      3         \n",
            "-----------------------------------------------------------------\n",
            "Overall         0.625      0.714      0.667      7         \n"
          ]
        }
      ],
      "source": [
        "class NERMetrics:\n",
        "    def __init__(self):\n",
        "        self.tp = defaultdict(int)  # True positives\n",
        "        self.fp = defaultdict(int)  # False positives\n",
        "        self.fn = defaultdict(int)  # False negatives\n",
        "\n",
        "    def extract_entities_from_bio(self, tokens: List[str], bio_tags: List[str]) -> List[Tuple[str, int, int, str]]:\n",
        "        \"\"\"Extract entities from BIO-tagged sequence\"\"\"\n",
        "        entities = []\n",
        "        current_entity = None\n",
        "\n",
        "        for i, (token, tag) in enumerate(zip(tokens, bio_tags)):\n",
        "            if tag.startswith('B-'):\n",
        "                # Begin new entity\n",
        "                if current_entity:\n",
        "                    entities.append(current_entity)\n",
        "                current_entity = (tag[2:], i, i, token)\n",
        "            elif tag.startswith('I-') and current_entity and tag[2:] == current_entity[0]:\n",
        "                # Continue current entity\n",
        "                current_entity = (current_entity[0], current_entity[1], i,\n",
        "                                current_entity[3] + ' ' + token)\n",
        "            else:\n",
        "                # End current entity\n",
        "                if current_entity:\n",
        "                    entities.append(current_entity)\n",
        "                    current_entity = None\n",
        "\n",
        "        if current_entity:\n",
        "            entities.append(current_entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def evaluate_sequence(self, tokens: List[str], true_tags: List[str], pred_tags: List[str]):\n",
        "        \"\"\"Evaluate a single sequence\"\"\"\n",
        "        true_entities = set(self.extract_entities_from_bio(tokens, true_tags))\n",
        "        pred_entities = set(self.extract_entities_from_bio(tokens, pred_tags))\n",
        "\n",
        "        # Count TP, FP, FN for each entity type\n",
        "        for entity in true_entities:\n",
        "            entity_type = entity[0]\n",
        "            if entity in pred_entities:\n",
        "                self.tp[entity_type] += 1\n",
        "            else:\n",
        "                self.fn[entity_type] += 1\n",
        "\n",
        "        for entity in pred_entities:\n",
        "            entity_type = entity[0]\n",
        "            if entity not in true_entities:\n",
        "                self.fp[entity_type] += 1\n",
        "\n",
        "    def compute_metrics(self) -> Dict[str, Dict[str, float]]:\n",
        "        \"\"\"Compute precision, recall, and F1 for each entity type\"\"\"\n",
        "        metrics = {}\n",
        "        all_types = set(self.tp.keys()) | set(self.fp.keys()) | set(self.fn.keys())\n",
        "\n",
        "        total_tp = total_fp = total_fn = 0\n",
        "\n",
        "        for entity_type in all_types:\n",
        "            tp = self.tp[entity_type]\n",
        "            fp = self.fp[entity_type]\n",
        "            fn = self.fn[entity_type]\n",
        "\n",
        "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            metrics[entity_type] = {\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'support': tp + fn\n",
        "            }\n",
        "\n",
        "            total_tp += tp\n",
        "            total_fp += fp\n",
        "            total_fn += fn\n",
        "\n",
        "        # Overall metrics\n",
        "        overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
        "        overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
        "        overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
        "\n",
        "        metrics['overall'] = {\n",
        "            'precision': overall_precision,\n",
        "            'recall': overall_recall,\n",
        "            'f1': overall_f1,\n",
        "            'support': total_tp + total_fn\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def print_report(self):\n",
        "        \"\"\"Print evaluation report\"\"\"\n",
        "        metrics = self.compute_metrics()\n",
        "\n",
        "        print(f\"{'Entity Type':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
        "        print(\"-\" * 65)\n",
        "\n",
        "        for entity_type, scores in metrics.items():\n",
        "            if entity_type != 'overall':\n",
        "                print(f\"{entity_type:<15} {scores['precision']:<10.3f} {scores['recall']:<10.3f} {scores['f1']:<10.3f} {scores['support']:<10}\")\n",
        "\n",
        "        print(\"-\" * 65)\n",
        "        overall = metrics['overall']\n",
        "        print(f\"{'Overall':<15} {overall['precision']:<10.3f} {overall['recall']:<10.3f} {overall['f1']:<10.3f} {overall['support']:<10}\")\n",
        "\n",
        "# Example evaluation\n",
        "evaluator = NERMetrics()\n",
        "\n",
        "# Sample data for evaluation\n",
        "test_cases = [\n",
        "    (\n",
        "        ['John', 'Smith', 'works', 'at', 'Google', 'in', 'California'],\n",
        "        ['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'O', 'B-LOC'],\n",
        "        ['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'O', 'B-LOC']  # Perfect prediction\n",
        "    ),\n",
        "    (\n",
        "        ['Apple', 'Inc', 'was', 'founded', 'by', 'Steve', 'Jobs'],\n",
        "        ['B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER'],\n",
        "        ['B-ORG', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER']  # Missed I-ORG\n",
        "    ),\n",
        "    (\n",
        "        ['Microsoft', 'is', 'in', 'Seattle', 'Washington'],\n",
        "        ['B-ORG', 'O', 'O', 'B-LOC', 'I-LOC'],\n",
        "        ['B-ORG', 'O', 'O', 'B-LOC', 'B-LOC']  # Wrong tag for Washington\n",
        "    )\n",
        "]\n",
        "\n",
        "for tokens, true_tags, pred_tags in test_cases:\n",
        "    evaluator.evaluate_sequence(tokens, true_tags, pred_tags)\n",
        "\n",
        "print(\"NER Evaluation Report:\")\n",
        "evaluator.print_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPoONSfKzOGn"
      },
      "source": [
        "## 6. Custom Entity Types and Domain Adaptation\n",
        "\n",
        "Creating NER systems for specific domains with custom entity types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DnyqmcLPzOGn",
        "outputId": "846eca2c-3453-4134-f520-253c2544dd9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Biomedical NER Results:\n",
            "  GENE: ['The', 'with', 'diagnosed', 'risk', 'and', 'was', 'gene', 'increases', 'mutation', 'prescribed', 'BRCA1', 'cancer', 'patient', 'metformin', 'diabetes', 'brca1']\n",
            "  PROTEIN: ['The', 'with', 'diagnosed', 'risk', 'and', 'was', 'gene', 'increases', 'mutation', 'prescribed', 'BRCA1', 'cancer', 'patient', 'metformin', 'diabetes']\n",
            "  DISEASE: ['cancer', 'diabetes', 'diabetes']\n",
            "  DOSAGE: ['500mg']\n",
            "\n",
            "Financial NER Results:\n",
            "  STOCK_SYMBOL: ['after', 'The', 'Apple', 'stock', 'rose', 'AAPL', 'to']\n",
            "  CURRENCY: ['$150.00']\n",
            "  FINANCIAL_TERM: ['dividend', 'earnings']\n",
            "  COMPANY: ['apple']\n",
            "  INSTRUMENT: ['stock']\n"
          ]
        }
      ],
      "source": [
        "class BiomedicalNER:\n",
        "    \"\"\"NER system specifically for biomedical domain\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Biomedical entity patterns\n",
        "        self.patterns = {\n",
        "            'GENE': r'\\b[A-Z][A-Z0-9]+\\b',  # Simple gene pattern\n",
        "            'PROTEIN': r'\\b[A-Z][a-z]+[0-9]*\\b',  # Simple protein pattern\n",
        "            'DISEASE': r'\\b(?:cancer|diabetes|hypertension|asthma|pneumonia)\\b',\n",
        "            'DRUG': r'\\b(?:aspirin|ibuprofen|penicillin|insulin|morphine)\\b',\n",
        "            'DOSAGE': r'\\b\\d+\\s*(?:mg|g|ml|cc|units?)\\b'\n",
        "        }\n",
        "\n",
        "        # Domain-specific dictionaries\n",
        "        self.gene_dict = {'BRCA1', 'BRCA2', 'TP53', 'EGFR', 'KRAS'}\n",
        "        self.protein_dict = {'insulin', 'hemoglobin', 'collagen', 'keratin'}\n",
        "        self.disease_dict = {'alzheimer', 'parkinson', 'huntington', 'diabetes'}\n",
        "        self.drug_dict = {'aspirin', 'metformin', 'lisinopril', 'atorvastatin'}\n",
        "\n",
        "    def extract_biomedical_entities(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract biomedical entities from text\"\"\"\n",
        "        entities = {}\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Pattern-based extraction\n",
        "        for entity_type, pattern in self.patterns.items():\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            if matches:\n",
        "                entities[entity_type] = list(set(matches))  # Remove duplicates\n",
        "\n",
        "        # Dictionary-based extraction\n",
        "        words = text_lower.split()\n",
        "\n",
        "        genes = [word for word in words if word.upper() in self.gene_dict]\n",
        "        if genes:\n",
        "            entities['GENE'] = entities.get('GENE', []) + genes\n",
        "\n",
        "        proteins = [word for word in words if word in self.protein_dict]\n",
        "        if proteins:\n",
        "            entities['PROTEIN'] = entities.get('PROTEIN', []) + proteins\n",
        "\n",
        "        diseases = [word for word in words if word in self.disease_dict]\n",
        "        if diseases:\n",
        "            entities['DISEASE'] = entities.get('DISEASE', []) + diseases\n",
        "\n",
        "        drugs = [word for word in words if word in self.drug_dict]\n",
        "        if drugs:\n",
        "            entities['DRUG'] = entities.get('DRUG', []) + drugs\n",
        "\n",
        "        return entities\n",
        "\n",
        "# Financial NER example\n",
        "class FinancialNER:\n",
        "    \"\"\"NER system for financial domain\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.patterns = {\n",
        "            'STOCK_SYMBOL': r'\\b[A-Z]{2,5}\\b',\n",
        "            'CURRENCY': r'\\$[0-9,]+(?:\\.[0-9]{2})?|USD|EUR|GBP',\n",
        "            'PERCENTAGE': r'\\b\\d+(?:\\.\\d+)?%\\b',\n",
        "            'FINANCIAL_TERM': r'\\b(?:IPO|merger|acquisition|dividend|earnings|revenue)\\b'\n",
        "        }\n",
        "\n",
        "        self.companies = {'apple', 'google', 'microsoft', 'amazon', 'tesla'}\n",
        "        self.financial_instruments = {'stock', 'bond', 'option', 'future', 'etf'}\n",
        "\n",
        "    def extract_financial_entities(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract financial entities from text\"\"\"\n",
        "        entities = {}\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        for entity_type, pattern in self.patterns.items():\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            if matches:\n",
        "                entities[entity_type] = list(set(matches))\n",
        "\n",
        "        words = text_lower.split()\n",
        "\n",
        "        companies = [word for word in words if word in self.companies]\n",
        "        if companies:\n",
        "            entities['COMPANY'] = companies\n",
        "\n",
        "        instruments = [word for word in words if word in self.financial_instruments]\n",
        "        if instruments:\n",
        "            entities['INSTRUMENT'] = instruments\n",
        "\n",
        "        return entities\n",
        "\n",
        "# Test domain-specific NER\n",
        "bio_ner = BiomedicalNER()\n",
        "fin_ner = FinancialNER()\n",
        "\n",
        "bio_text = \"The patient was diagnosed with diabetes and prescribed 500mg metformin. BRCA1 gene mutation increases cancer risk.\"\n",
        "fin_text = \"Apple stock (AAPL) rose 5.2% to $150.00 after strong earnings report. The company announced a dividend increase.\"\n",
        "\n",
        "print(\"Biomedical NER Results:\")\n",
        "bio_entities = bio_ner.extract_biomedical_entities(bio_text)\n",
        "for ent_type, entities in bio_entities.items():\n",
        "    print(f\"  {ent_type}: {entities}\")\n",
        "\n",
        "print(\"\\nFinancial NER Results:\")\n",
        "fin_entities = fin_ner.extract_financial_entities(fin_text)\n",
        "for ent_type, entities in fin_entities.items():\n",
        "    print(f\"  {ent_type}: {entities}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcFxmO4xzOGn"
      },
      "source": [
        "---\n",
        "\n",
        "# NER Challenges\n",
        "\n",
        "Test your understanding with these progressive challenges!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkKDsiDAzOGn"
      },
      "source": [
        "\n",
        "### Challenge 1: Pattern Enhancement\n",
        "Enhance the `RuleBasedNER` class with better patterns for:\n",
        "- Social Security Numbers (XXX-XX-XXXX)\n",
        "- IP Addresses (XXX.XXX.XXX.XXX)\n",
        "- Credit Card Numbers (XXXX-XXXX-XXXX-XXXX)\n",
        "\n",
        "**Success Criteria:**\n",
        "- Add at least 3 new entity patterns\n",
        "- Test with sample text containing these entities\n",
        "- Achieve 90%+ precision on test cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QFN4j1yLzOGo"
      },
      "outputs": [],
      "source": [
        "# Your solution for Challenge 1\n",
        "class EnhancedRuleBasedNER(RuleBasedNER):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TODO: Add new patterns for SSN, IP addresses, and credit card numbers\n",
        "        pass\n",
        "\n",
        "# Test your enhanced NER\n",
        "test_text = \"\"\"\n",
        "Contact info: SSN 123-45-6789, IP address 192.168.1.1,\n",
        "Credit card 1234-5678-9012-3456 expires 12/25\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Test your implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iTBaD-czOGo"
      },
      "source": [
        "### Challenge 2: Entity Linking\n",
        "Create a simple entity linking system that maps recognized entities to knowledge base entries.\n",
        "\n",
        "**Requirements:**\n",
        "- Create a knowledge base with entity information\n",
        "- Link recognized entities to KB entries\n",
        "- Handle entity disambiguation\n",
        "\n",
        "**Success Criteria:**\n",
        "- Successfully link at least 80% of entities\n",
        "- Handle ambiguous entities (e.g., \"Apple\" company vs fruit)\n",
        "- Provide confidence scores for links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3Zy9Di3HzOGo"
      },
      "outputs": [],
      "source": [
        "# Your solution for Challenge 2\n",
        "class EntityLinker:\n",
        "    def __init__(self):\n",
        "        # TODO: Create knowledge base\n",
        "        self.knowledge_base = {}\n",
        "\n",
        "    def link_entities(self, entities, context):\n",
        "        # TODO: Implement entity linking logic\n",
        "        pass\n",
        "\n",
        "# Test entity linking\n",
        "test_entities = [('Apple', 'ORG'), ('New York', 'LOC'), ('Smith', 'PER')]\n",
        "context = \"Apple Inc. announced new products in New York where John Smith presented.\"\n",
        "\n",
        "# TODO: Test your implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0_c2sjHzOGo"
      },
      "source": [
        "\n",
        "\n",
        "### Challenge 3: Multi-language NER\n",
        "Extend the NER system to handle multiple languages (English, Spanish, French).\n",
        "\n",
        "**Requirements:**\n",
        "- Detect language of input text\n",
        "- Use language-specific patterns and dictionaries\n",
        "- Handle code-switching (mixed languages)\n",
        "\n",
        "**Success Criteria:**\n",
        "- Support at least 3 languages\n",
        "- Achieve 75%+ F1 score on multilingual test set\n",
        "- Handle mixed-language sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7b0NNanczOGo"
      },
      "outputs": [],
      "source": [
        "# Your solution for Challenge 3\n",
        "class MultilingualNER:\n",
        "    def __init__(self):\n",
        "        # TODO: Initialize language-specific resources\n",
        "        self.language_patterns = {}\n",
        "        self.language_dictionaries = {}\n",
        "\n",
        "    def detect_language(self, text):\n",
        "        # TODO: Implement language detection\n",
        "        pass\n",
        "\n",
        "    def extract_entities_multilingual(self, text):\n",
        "        # TODO: Extract entities considering language\n",
        "        pass\n",
        "\n",
        "# Test multilingual NER\n",
        "test_texts = [\n",
        "    \"Apple Inc. is located in California.\",  # English\n",
        "    \"Juan Garca trabaja en Madrid, Espaa.\",  # Spanish\n",
        "    \"Marie Dupont vit  Paris, France.\",  # French\n",
        "]\n",
        "\n",
        "# TODO: Test your implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCJtfVBjzOGo"
      },
      "source": [
        "### Challenge 4: Active Learning for NER\n",
        "Implement an active learning system that selects the most informative examples for annotation.\n",
        "\n",
        "**Requirements:**\n",
        "- Implement uncertainty sampling\n",
        "- Create annotation interface simulation\n",
        "- Update model with new annotations\n",
        "\n",
        "**Success Criteria:**\n",
        "- Reduce annotation effort by 40% compared to random sampling\n",
        "- Achieve target performance with fewer labeled examples\n",
        "- Implement at least 2 query strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZbhzQFrfzOGo"
      },
      "outputs": [],
      "source": [
        "# Your solution for Challenge 4\n",
        "class ActiveLearningNER:\n",
        "    def __init__(self, base_model):\n",
        "        self.model = base_model\n",
        "        self.labeled_data = []\n",
        "        self.unlabeled_data = []\n",
        "\n",
        "    def uncertainty_sampling(self, candidates, n_samples=5):\n",
        "        # TODO: Select most uncertain examples\n",
        "        pass\n",
        "\n",
        "    def diversity_sampling(self, candidates, n_samples=5):\n",
        "        # TODO: Select diverse examples\n",
        "        pass\n",
        "\n",
        "    def update_model(self, new_annotations):\n",
        "        # TODO: Retrain model with new data\n",
        "        pass\n",
        "\n",
        "# TODO: Implement and test active learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycNWqGVtzOGo"
      },
      "source": [
        "\n",
        "\n",
        "### Challenge 5: Nested Entity Recognition\n",
        "Build a system that can recognize nested entities (entities within entities).\n",
        "\n",
        "**Example:**\n",
        "- \"University of California, Berkeley\" contains:\n",
        "  - ORGANIZATION: \"University of California, Berkeley\"\n",
        "  - LOCATION: \"California\"\n",
        "  - LOCATION: \"Berkeley\"\n",
        "\n",
        "**Requirements:**\n",
        "- Handle overlapping entity spans\n",
        "- Maintain entity hierarchy\n",
        "- Resolve entity boundaries conflicts\n",
        "\n",
        "**Success Criteria:**\n",
        "- Successfully identify nested entities in 85% of cases\n",
        "- Maintain hierarchy relationships\n",
        "- Handle at least 3 levels of nesting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5C-hrjjSzOGo"
      },
      "outputs": [],
      "source": [
        "# Your solution for Challenge 5\n",
        "from typing import List, Tuple, Dict, Set\n",
        "\n",
        "class NestedNER:\n",
        "    def __init__(self):\n",
        "        # TODO: Initialize nested entity recognition system\n",
        "        pass\n",
        "\n",
        "    def find_nested_entities(self, text: str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Return format: [\n",
        "            {\n",
        "                'text': 'entity text',\n",
        "                'start': start_pos,\n",
        "                'end': end_pos,\n",
        "                'type': 'ENTITY_TYPE',\n",
        "                'children': [nested_entities],\n",
        "                'parent': parent_entity_id\n",
        "            }\n",
        "        ]\n",
        "        \"\"\"\n",
        "        # TODO: Implement nested entity extraction\n",
        "        pass\n",
        "\n",
        "    def resolve_conflicts(self, entities: List[Dict]) -> List[Dict]:\n",
        "        # TODO: Resolve overlapping entity conflicts\n",
        "        pass\n",
        "\n",
        "# Test nested NER\n",
        "test_cases = [\n",
        "    \"University of California, Berkeley\",\n",
        "    \"New York City, New York, United States\",\n",
        "    \"Apple Inc. CEO Tim Cook from California\"\n",
        "]\n",
        "\n",
        "# TODO: Test your implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlWtRha-zOGo"
      },
      "source": [
        "### Challenge 6: Few-Shot NER with Meta-Learning\n",
        "Implement a few-shot learning system that can quickly adapt to new entity types with minimal examples.\n",
        "\n",
        "**Requirements:**\n",
        "- Implement meta-learning algorithm (MAML or similar)\n",
        "- Support for new entity types with 5-10 examples\n",
        "- Fast adaptation mechanism\n",
        "\n",
        "**Success Criteria:**\n",
        "- Achieve 70%+ F1 on new entity types with 5 examples\n",
        "- Adaptation time < 1 minute\n",
        "- Support at least 5 different domain adaptations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hXtOoGBEzOGo"
      },
      "outputs": [],
      "source": [
        "# Your solution for Challenge 6\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "class FewShotNER:\n",
        "    def __init__(self, embedding_dim=128, hidden_dim=256):\n",
        "        # TODO: Initialize meta-learning NER model\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.model = None  # TODO: Define model architecture\n",
        "\n",
        "    def meta_train(self, support_sets, query_sets, n_epochs=100):\n",
        "        # TODO: Implement MAML training\n",
        "        pass\n",
        "\n",
        "    def fast_adapt(self, support_examples, n_steps=5):\n",
        "        # TODO: Fast adaptation to new entity type\n",
        "        pass\n",
        "\n",
        "    def predict_new_domain(self, text, entity_type):\n",
        "        # TODO: Predict entities of new type\n",
        "        pass\n",
        "\n",
        "# TODO: Implement and test few-shot NER"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}