{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models and Text Generation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "At the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand different types of language models\n",
    "2. Implement n-gram language models from scratch\n",
    "3. Build neural language models using RNNs and Transformers\n",
    "4. Generate text using various decoding strategies\n",
    "5. Fine-tune pre-trained language models\n",
    "6. Evaluate text generation quality\n",
    "7. Handle different text generation tasks\n",
    "8. Deploy language models for production use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Language Models\n",
    "\n",
    "Language Models (LMs) are statistical models that learn the probability distribution over sequences of words, characters, or tokens. They capture the patterns and structure of natural language and can predict the likelihood of a sequence of text.\n",
    "\n",
    "### Types of Language Models:\n",
    "\n",
    "1. **Statistical Models**: N-gram models, Hidden Markov Models\n",
    "2. **Neural Models**: RNN-based, LSTM/GRU, Transformer-based\n",
    "3. **Pre-trained Models**: GPT, BERT, T5, etc.\n",
    "4. **Autoregressive Models**: Generate text sequentially (GPT family)\n",
    "5. **Autoencoding Models**: Encode input to generate output (BERT family)\n",
    "6. **Sequence-to-Sequence Models**: Encoder-decoder architectures (T5, BART)\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- **Text Generation**: Creative writing, code generation\n",
    "- **Text Completion**: Auto-complete, predictive text\n",
    "- **Machine Translation**: Language-to-language translation\n",
    "- **Text Summarization**: Abstractive and extractive summaries\n",
    "- **Dialogue Systems**: Chatbots and conversational AI\n",
    "- **Code Generation**: Programming assistance\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Perplexity**: Measure of how well the model predicts text\n",
    "- **Decoding Strategies**: Greedy, beam search, sampling\n",
    "- **Fine-tuning**: Adapting pre-trained models to specific tasks\n",
    "- **Prompt Engineering**: Designing inputs for better outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages",
    "!pip install numpy pandas matplotlib seaborn nltk torch transformers tqdm scikit-learn",
    "",
    "# Download required NLTK data",
    "import nltk",
    "for item in ['punkt', 'stopwords']:",
    "    nltk.download(item, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformers library (if available)\n",
    "try:\n",
    "    from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"Note: transformers library not available. Some features will be limited.\")\n",
    "\n",
    "# NLTK for text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Transformers available: {TRANSFORMERS_AVAILABLE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. N-gram Language Models\n",
    "\n",
    "Starting with the fundamentals: statistical n-gram models that predict the next word based on the previous n-1 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, n: int = 3, smoothing: str = 'laplace', alpha: float = 1.0):\n",
    "        self.n = n\n",
    "        self.smoothing = smoothing\n",
    "        self.alpha = alpha  # Smoothing parameter\n",
    "        \n",
    "        # Storage for n-gram counts\n",
    "        self.ngram_counts = defaultdict(int)\n",
    "        self.context_counts = defaultdict(int)\n",
    "        self.vocabulary = set()\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "        # Special tokens\n",
    "        self.start_token = '<START>'\n",
    "        self.end_token = '<END>'\n",
    "        self.unk_token = '<UNK>'\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Preprocess text into tokens\"\"\"\n",
    "        # Basic cleaning\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s.,!?;:]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def get_ngrams(self, tokens: List[str]) -> List[Tuple[str, ...]]:\n",
    "        \"\"\"Extract n-grams from token sequence\"\"\"\n",
    "        # Add start and end tokens\n",
    "        padded_tokens = [self.start_token] * (self.n - 1) + tokens + [self.end_token]\n",
    "        \n",
    "        ngrams = []\n",
    "        for i in range(len(padded_tokens) - self.n + 1):\n",
    "            ngram = tuple(padded_tokens[i:i + self.n])\n",
    "            ngrams.append(ngram)\n",
    "        \n",
    "        return ngrams\n",
    "    \n",
    "    def train(self, texts: List[str]):\n",
    "        \"\"\"Train the n-gram model on a corpus\"\"\"\n",
    "        print(f\"Training {self.n}-gram model on {len(texts)} texts...\")\n",
    "        \n",
    "        all_tokens = []\n",
    "        \n",
    "        # Process all texts\n",
    "        for text in texts:\n",
    "            tokens = self.preprocess_text(text)\n",
    "            all_tokens.extend(tokens)\n",
    "            \n",
    "            # Get n-grams\n",
    "            ngrams = self.get_ngrams(tokens)\n",
    "            \n",
    "            # Count n-grams and contexts\n",
    "            for ngram in ngrams:\n",
    "                context = ngram[:-1]\n",
    "                word = ngram[-1]\n",
    "                \n",
    "                self.ngram_counts[ngram] += 1\n",
    "                self.context_counts[context] += 1\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.vocabulary = set(all_tokens)\n",
    "        self.vocabulary.add(self.start_token)\n",
    "        self.vocabulary.add(self.end_token)\n",
    "        self.vocabulary.add(self.unk_token)\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        \n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Total n-grams: {len(self.ngram_counts)}\")\n",
    "    \n",
    "    def get_probability(self, context: Tuple[str, ...], word: str) -> float:\n",
    "        \"\"\"Calculate probability of word given context\"\"\"\n",
    "        ngram = context + (word,)\n",
    "        \n",
    "        if self.smoothing == 'laplace':\n",
    "            # Laplace smoothing\n",
    "            numerator = self.ngram_counts[ngram] + self.alpha\n",
    "            denominator = self.context_counts[context] + self.alpha * self.vocab_size\n",
    "            \n",
    "            return numerator / denominator if denominator > 0 else 1e-10\n",
    "        \n",
    "        elif self.smoothing == 'kneser_ney':\n",
    "            # Simplified Kneser-Ney smoothing\n",
    "            d = 0.75  # Discount parameter\n",
    "            \n",
    "            count = self.ngram_counts[ngram]\n",
    "            context_count = self.context_counts[context]\n",
    "            \n",
    "            if context_count == 0:\n",
    "                return 1e-10\n",
    "            \n",
    "            # Continuation count (simplified)\n",
    "            continuation_count = sum(1 for ng in self.ngram_counts if ng[-1] == word)\n",
    "            total_continuations = len(set(ng[-1] for ng in self.ngram_counts))\n",
    "            \n",
    "            prob = max(count - d, 0) / context_count\n",
    "            lambda_weight = d / context_count * len([ng for ng in self.ngram_counts if ng[:-1] == context])\n",
    "            backoff_prob = continuation_count / total_continuations if total_continuations > 0 else 1e-10\n",
    "            \n",
    "            return prob + lambda_weight * backoff_prob\n",
    "        \n",
    "        else:\n",
    "            # Maximum likelihood estimation (no smoothing)\n",
    "            if self.context_counts[context] == 0:\n",
    "                return 1e-10\n",
    "            return self.ngram_counts[ngram] / self.context_counts[context]\n",
    "    \n",
    "    def generate_text(self, prompt: str = \"\", max_length: int = 50, temperature: float = 1.0) -> str:\n",
    "        \"\"\"Generate text using the trained model\"\"\"\n",
    "        if prompt:\n",
    "            tokens = self.preprocess_text(prompt)\n",
    "        else:\n",
    "            tokens = []\n",
    "        \n",
    "        # Ensure we have enough context\n",
    "        if len(tokens) < self.n - 1:\n",
    "            tokens = [self.start_token] * (self.n - 1 - len(tokens)) + tokens\n",
    "        \n",
    "        generated_tokens = tokens.copy()\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Get current context\n",
    "            context = tuple(generated_tokens[-(self.n-1):])\n",
    "            \n",
    "            # Calculate probabilities for all possible next words\n",
    "            word_probs = {}\n",
    "            for word in self.vocabulary:\n",
    "                if word not in [self.start_token, self.unk_token]:\n",
    "                    prob = self.get_probability(context, word)\n",
    "                    word_probs[word] = prob\n",
    "            \n",
    "            if not word_probs:\n",
    "                break\n",
    "            \n",
    "            # Sample next word\n",
    "            if temperature == 0:\n",
    "                # Greedy selection\n",
    "                next_word = max(word_probs, key=word_probs.get)\n",
    "            else:\n",
    "                # Temperature sampling\n",
    "                words = list(word_probs.keys())\n",
    "                probs = np.array(list(word_probs.values()))\n",
    "                \n",
    "                # Apply temperature\n",
    "                probs = np.exp(np.log(probs + 1e-10) / temperature)\n",
    "                probs = probs / np.sum(probs)\n",
    "                \n",
    "                next_word = np.random.choice(words, p=probs)\n",
    "            \n",
    "            if next_word == self.end_token:\n",
    "                break\n",
    "            \n",
    "            generated_tokens.append(next_word)\n",
    "        \n",
    "        # Remove start tokens and join\n",
    "        generated_text = ' '.join([token for token in generated_tokens \n",
    "                                 if token != self.start_token])\n",
    "        \n",
    "        return generated_text\n",
    "    \n",
    "    def calculate_perplexity(self, test_texts: List[str]) -> float:\n",
    "        \"\"\"Calculate perplexity on test data\"\"\"\n",
    "        total_log_prob = 0\n",
    "        total_words = 0\n",
    "        \n",
    "        for text in test_texts:\n",
    "            tokens = self.preprocess_text(text)\n",
    "            ngrams = self.get_ngrams(tokens)\n",
    "            \n",
    "            for ngram in ngrams:\n",
    "                context = ngram[:-1]\n",
    "                word = ngram[-1]\n",
    "                \n",
    "                prob = self.get_probability(context, word)\n",
    "                total_log_prob += math.log(prob)\n",
    "                total_words += 1\n",
    "        \n",
    "        if total_words == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        avg_log_prob = total_log_prob / total_words\n",
    "        perplexity = math.exp(-avg_log_prob)\n",
    "        \n",
    "        return perplexity\n",
    "\n",
    "# Create sample training corpus\n",
    "training_corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"A journey of a thousand miles begins with a single step.\",\n",
    "    \"To be or not to be, that is the question.\",\n",
    "    \"All that glitters is not gold.\",\n",
    "    \"The early bird catches the worm.\",\n",
    "    \"Actions speak louder than words.\",\n",
    "    \"Better late than never.\",\n",
    "    \"Don't count your chickens before they hatch.\",\n",
    "    \"Every cloud has a silver lining.\",\n",
    "    \"Fortune favors the bold.\",\n",
    "    \"Good things come to those who wait.\",\n",
    "    \"Hope for the best, prepare for the worst.\",\n",
    "    \"If you can't beat them, join them.\",\n",
    "    \"Knowledge is power.\",\n",
    "    \"Laughter is the best medicine.\",\n",
    "    \"Money can't buy happiness.\",\n",
    "    \"No pain, no gain.\",\n",
    "    \"Opportunity knocks but once.\",\n",
    "    \"Practice makes perfect.\",\n",
    "    \"Rome wasn't built in a day.\"\n",
    "]\n",
    "\n",
    "# Train different n-gram models\n",
    "models = {}\n",
    "for n in [2, 3, 4]:\n",
    "    print(f\"\\nTraining {n}-gram model:\")\n",
    "    model = NGramLanguageModel(n=n, smoothing='laplace')\n",
    "    model.train(training_corpus)\n",
    "    models[n] = model\n",
    "\n",
    "# Generate text with different models\n",
    "print(\"\\nText Generation Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for n, model in models.items():\n",
    "    print(f\"\\n{n}-gram model:\")\n",
    "    for temp in [0.5, 1.0, 1.5]:\n",
    "        generated = model.generate_text(prompt=\"the\", max_length=15, temperature=temp)\n",
    "        print(f\"  Temperature {temp}: {generated}\")\n",
    "\n",
    "# Calculate perplexity\n",
    "test_corpus = [\n",
    "    \"The wise owl sits in the oak.\",\n",
    "    \"Time waits for no one.\",\n",
    "    \"A penny saved is a penny earned.\"\n",
    "]\n",
    "\n",
    "print(\"\\nPerplexity on test corpus:\")\n",
    "print(\"=\" * 30)\n",
    "for n, model in models.items():\n",
    "    perplexity = model.calculate_perplexity(test_corpus)\n",
    "    print(f\"{n}-gram model: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Language Models\n",
    "\n",
    "Implementing RNN-based language models that can capture longer dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], vocab: Dict[str, int], sequence_length: int = 50):\n",
    "        self.texts = texts\n",
    "        self.vocab = vocab\n",
    "        self.sequence_length = sequence_length\n",
    "        self.sequences = self._create_sequences()\n",
    "    \n",
    "    def _create_sequences(self):\n",
    "        \"\"\"Create training sequences from texts\"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        for text in self.texts:\n",
    "            # Tokenize\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            \n",
    "            # Convert to indices\n",
    "            indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "            \n",
    "            # Create sequences\n",
    "            for i in range(len(indices) - self.sequence_length):\n",
    "                input_seq = indices[i:i + self.sequence_length]\n",
    "                target_seq = indices[i + 1:i + self.sequence_length + 1]\n",
    "                sequences.append((input_seq, target_seq))\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target_seq = self.sequences[idx]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target_seq, dtype=torch.long)\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int = 128, hidden_dim: int = 256, \n",
    "                 num_layers: int = 2, rnn_type: str = 'LSTM', dropout: float = 0.3):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "        # Layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                              dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, \n",
    "                             dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, \n",
    "                             dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
    "        \n",
    "        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids, hidden=None):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(input_ids)  # (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # RNN\n",
    "        output, hidden = self.rnn(embedded, hidden)  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        logits = self.output_projection(output)  # (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        return logits, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"Initialize hidden state\"\"\"\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "            return (h0, c0)\n",
    "        else:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "            return h0\n",
    "\n",
    "class NeuralLanguageModelTrainer:\n",
    "    def __init__(self, model, vocab, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "        self.idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "    \n",
    "    def train(self, train_loader, val_loader=None, epochs=10, lr=0.001):\n",
    "        \"\"\"Train the neural language model\"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            total_train_loss = 0\n",
    "            \n",
    "            for batch_idx, (input_ids, target_ids) in enumerate(train_loader):\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                target_ids = target_ids.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                hidden = self.model.init_hidden(input_ids.size(0), self.device)\n",
    "                logits, _ = self.model(input_ids, hidden)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            if val_loader:\n",
    "                self.model.eval()\n",
    "                total_val_loss = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for input_ids, target_ids in val_loader:\n",
    "                        input_ids = input_ids.to(self.device)\n",
    "                        target_ids = target_ids.to(self.device)\n",
    "                        \n",
    "                        hidden = self.model.init_hidden(input_ids.size(0), self.device)\n",
    "                        logits, _ = self.model(input_ids, hidden)\n",
    "                        \n",
    "                        loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "                        total_val_loss += loss.item()\n",
    "                \n",
    "                avg_val_loss = total_val_loss / len(val_loader)\n",
    "                val_losses.append(avg_val_loss)\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {avg_train_loss:.4f}\")\n",
    "        \n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def generate_text(self, prompt: str = \"\", max_length: int = 50, temperature: float = 1.0, \n",
    "                     top_k: int = None, top_p: float = None) -> str:\n",
    "        \"\"\"Generate text using various decoding strategies\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        if prompt:\n",
    "            tokens = word_tokenize(prompt.lower())\n",
    "            input_ids = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "        else:\n",
    "            input_ids = [self.vocab.get('<START>', 0)]\n",
    "        \n",
    "        input_ids = torch.tensor([input_ids]).to(self.device)\n",
    "        hidden = self.model.init_hidden(1, self.device)\n",
    "        \n",
    "        generated_tokens = input_ids[0].tolist()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                # Forward pass\n",
    "                logits, hidden = self.model(input_ids, hidden)\n",
    "                \n",
    "                # Get next token logits\n",
    "                next_token_logits = logits[0, -1, :] / temperature\n",
    "                \n",
    "                # Apply top-k filtering\n",
    "                if top_k is not None:\n",
    "                    top_k = min(top_k, next_token_logits.size(-1))\n",
    "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "                    next_token_logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                # Apply top-p (nucleus) filtering\n",
    "                if top_p is not None:\n",
    "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    \n",
    "                    # Remove tokens with cumulative probability above the threshold\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = 0\n",
    "                    \n",
    "                    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                    next_token_logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                # Sample next token\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                generated_tokens.append(next_token.item())\n",
    "                \n",
    "                # Update input for next iteration\n",
    "                input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "                \n",
    "                # Stop if end token is generated\n",
    "                if next_token.item() == self.vocab.get('<END>', -1):\n",
    "                    break\n",
    "        \n",
    "        # Convert back to text\n",
    "        generated_text = ' '.join([self.idx_to_word.get(idx, '<UNK>') for idx in generated_tokens\n",
    "                                  if idx not in [self.vocab.get('<START>', -1), self.vocab.get('<END>', -1)]])\n",
    "        \n",
    "        return generated_text\n",
    "    \n",
    "    def calculate_perplexity(self, test_loader):\n",
    "        \"\"\"Calculate perplexity on test data\"\"\"\n",
    "        self.model.eval()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for input_ids, target_ids in test_loader:\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                target_ids = target_ids.to(self.device)\n",
    "                \n",
    "                hidden = self.model.init_hidden(input_ids.size(0), self.device)\n",
    "                logits, _ = self.model(input_ids, hidden)\n",
    "                \n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "                total_loss += loss.item() * target_ids.numel()\n",
    "                total_tokens += target_ids.numel()\n",
    "        \n",
    "        avg_loss = total_loss / total_tokens\n",
    "        perplexity = math.exp(avg_loss)\n",
    "        \n",
    "        return perplexity\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocabulary(texts: List[str], min_freq: int = 2):\n",
    "    \"\"\"Build vocabulary from text corpus\"\"\"\n",
    "    word_freq = Counter()\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        word_freq.update(tokens)\n",
    "    \n",
    "    # Create vocabulary\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1, '<START>': 2, '<END>': 3}\n",
    "    idx = 4\n",
    "    \n",
    "    for word, freq in word_freq.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Extended training corpus for neural model\n",
    "extended_corpus = training_corpus + [\n",
    "    \"Success is not final, failure is not fatal: it is the courage to continue that counts.\",\n",
    "    \"The only way to do great work is to love what you do.\",\n",
    "    \"Innovation distinguishes between a leader and a follower.\",\n",
    "    \"The future belongs to those who believe in the beauty of their dreams.\",\n",
    "    \"It is during our darkest moments that we must focus to see the light.\",\n",
    "    \"Whether you think you can or you think you can't, you're right.\",\n",
    "    \"The way to get started is to quit talking and begin doing.\",\n",
    "    \"Don't let yesterday take up too much of today.\",\n",
    "    \"You learn more from failure than from success.\",\n",
    "    \"If you are not willing to risk the usual, you will have to settle for the ordinary.\"\n",
    "]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocabulary(extended_corpus)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(extended_corpus, vocab, sequence_length=20)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "print(f\"Training sequences: {len(train_dataset)}\")\n",
    "\n",
    "# Initialize model and trainer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RNNLanguageModel(vocab_size=len(vocab), embedding_dim=64, hidden_dim=128, \n",
    "                        num_layers=2, rnn_type='LSTM')\n",
    "trainer = NeuralLanguageModelTrainer(model, vocab, device)\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining neural language model...\")\n",
    "train_losses, _ = trainer.train(train_loader, epochs=5, lr=0.01)\n",
    "\n",
    "# Generate text\n",
    "print(\"\\nNeural Language Model Text Generation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "generation_strategies = [\n",
    "    {'temperature': 0.5, 'top_k': None, 'top_p': None, 'name': 'Low Temperature'},\n",
    "    {'temperature': 1.0, 'top_k': None, 'top_p': None, 'name': 'Normal Temperature'},\n",
    "    {'temperature': 1.0, 'top_k': 10, 'top_p': None, 'name': 'Top-K Sampling'},\n",
    "    {'temperature': 1.0, 'top_k': None, 'top_p': 0.9, 'name': 'Top-P Sampling'}\n",
    "]\n",
    "\n",
    "for strategy in generation_strategies:\n",
    "    generated = trainer.generate_text(\n",
    "        prompt=\"the\", \n",
    "        max_length=20, \n",
    "        temperature=strategy['temperature'],\n",
    "        top_k=strategy['top_k'],\n",
    "        top_p=strategy['top_p']\n",
    "    )\n",
    "    print(f\"{strategy['name']}: {generated}\")\n",
    "\n",
    "# Calculate perplexity\n",
    "test_dataset = TextDataset(test_corpus, vocab, sequence_length=20)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "if len(test_dataset) > 0:\n",
    "    perplexity = trainer.calculate_perplexity(test_loader)\n",
    "    print(f\"\\nNeural model perplexity: {perplexity:.2f}\")\n",
    "else:\n",
    "    print(\"\\nNot enough test data for perplexity calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer-based Language Models\n",
    "\n",
    "Implementing a simple transformer architecture for language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear transformations and reshape\n",
    "        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear transformation\n",
    "        output = self.w_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_length: int = 5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, _ = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int = 256, num_heads: int = 8, \n",
    "                 num_layers: int = 6, d_ff: int = 1024, max_length: int = 512, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_length)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def create_causal_mask(self, size):\n",
    "        \"\"\"Create causal mask for autoregressive generation\"\"\"\n",
    "        mask = torch.tril(torch.ones(size, size))\n",
    "        return mask.unsqueeze(0).unsqueeze(0)  # Add batch and head dimensions\n",
    "    \n",
    "    def forward(self, input_ids, targets=None):\n",
    "        batch_size, seq_length = input_ids.size()\n",
    "        \n",
    "        # Token embeddings\n",
    "        token_embeddings = self.token_embedding(input_ids)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.dropout(self.positional_encoding(token_embeddings))\n",
    "        \n",
    "        # Create causal mask\n",
    "        causal_mask = self.create_causal_mask(seq_length).to(input_ids.device)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, causal_mask)\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Calculate cross-entropy loss\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "class TransformerTrainer:\n",
    "    def __init__(self, model, vocab, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "        self.idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "    \n",
    "    def train(self, train_loader, val_loader=None, epochs=10, lr=1e-4):\n",
    "        \"\"\"Train the transformer model\"\"\"\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr, betas=(0.9, 0.95))\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            total_train_loss = 0\n",
    "            \n",
    "            for batch_idx, (input_ids, target_ids) in enumerate(train_loader):\n",
    "                input_ids = input_ids.to(self.device)\n",
    "                target_ids = target_ids.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                logits, loss = self.model(input_ids, target_ids)\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            if val_loader:\n",
    "                self.model.eval()\n",
    "                total_val_loss = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for input_ids, target_ids in val_loader:\n",
    "                        input_ids = input_ids.to(self.device)\n",
    "                        target_ids = target_ids.to(self.device)\n",
    "                        \n",
    "                        logits, loss = self.model(input_ids, target_ids)\n",
    "                        total_val_loss += loss.item()\n",
    "                \n",
    "                avg_val_loss = total_val_loss / len(val_loader)\n",
    "                val_losses.append(avg_val_loss)\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {avg_train_loss:.4f}\")\n",
    "            \n",
    "            scheduler.step()\n",
    "        \n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def generate_text(self, prompt: str = \"\", max_length: int = 50, temperature: float = 1.0,\n",
    "                     top_k: int = None, top_p: float = None) -> str:\n",
    "        \"\"\"Generate text using the transformer model\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        if prompt:\n",
    "            tokens = word_tokenize(prompt.lower())\n",
    "            input_ids = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "        else:\n",
    "            input_ids = [self.vocab.get('<START>', 0)]\n",
    "        \n",
    "        input_ids = torch.tensor([input_ids]).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                # Forward pass\n",
    "                logits, _ = self.model(input_ids)\n",
    "                \n",
    "                # Get next token logits\n",
    "                next_token_logits = logits[0, -1, :] / temperature\n",
    "                \n",
    "                # Apply filtering strategies (same as RNN model)\n",
    "                if top_k is not None:\n",
    "                    top_k = min(top_k, next_token_logits.size(-1))\n",
    "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "                    next_token_logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                if top_p is not None:\n",
    "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    \n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = 0\n",
    "                    \n",
    "                    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                    next_token_logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                # Sample next token\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                # Append to sequence\n",
    "                input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "                \n",
    "                # Stop if end token or max length reached\n",
    "                if next_token.item() == self.vocab.get('<END>', -1):\n",
    "                    break\n",
    "        \n",
    "        # Convert back to text\n",
    "        generated_tokens = input_ids[0].tolist()\n",
    "        generated_text = ' '.join([self.idx_to_word.get(idx, '<UNK>') for idx in generated_tokens\n",
    "                                  if idx not in [self.vocab.get('<START>', -1), self.vocab.get('<END>', -1)]])\n",
    "        \n",
    "        return generated_text\n",
    "\n",
    "# Initialize transformer model (smaller for demo)\n",
    "transformer_model = TransformerLanguageModel(\n",
    "    vocab_size=len(vocab),\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    num_layers=3,\n",
    "    d_ff=256,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "transformer_trainer = TransformerTrainer(transformer_model, vocab, device)\n",
    "\n",
    "print(f\"\\nTransformer model parameters: {sum(p.numel() for p in transformer_model.parameters()):,}\")\n",
    "\n",
    "# Train transformer (reduced epochs for demo)\n",
    "print(\"\\nTraining transformer model...\")\n",
    "transformer_losses, _ = transformer_trainer.train(train_loader, epochs=3, lr=5e-4)\n",
    "\n",
    "# Generate text with transformer\n",
    "print(\"\\nTransformer Text Generation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for strategy in generation_strategies:\n",
    "    generated = transformer_trainer.generate_text(\n",
    "        prompt=\"the future\",\n",
    "        max_length=15,\n",
    "        temperature=strategy['temperature'],\n",
    "        top_k=strategy['top_k'],\n",
    "        top_p=strategy['top_p']\n",
    "    )\n",
    "    print(f\"{strategy['name']}: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Generation Evaluation\n",
    "\n",
    "Implementing various metrics to evaluate the quality of generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerationEvaluator:\n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def calculate_bleu_score(self, generated_text: str, reference_texts: List[str], n: int = 4) -> float:\n",
    "        \"\"\"Calculate BLEU score for generated text\"\"\"\n",
    "        from collections import Counter\n",
    "        \n",
    "        def get_ngrams(text: str, n: int) -> List[tuple]:\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "        \n",
    "        # Get n-grams for generated and reference texts\n",
    "        gen_ngrams = get_ngrams(generated_text, n)\n",
    "        ref_ngrams_list = [get_ngrams(ref, n) for ref in reference_texts]\n",
    "        \n",
    "        if not gen_ngrams:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate precision for each n-gram length\n",
    "        precisions = []\n",
    "        \n",
    "        for i in range(1, n + 1):\n",
    "            gen_ngrams_i = get_ngrams(generated_text, i)\n",
    "            gen_counts = Counter(gen_ngrams_i)\n",
    "            \n",
    "            max_counts = Counter()\n",
    "            for ref_text in reference_texts:\n",
    "                ref_ngrams_i = get_ngrams(ref_text, i)\n",
    "                ref_counts = Counter(ref_ngrams_i)\n",
    "                for ngram in gen_counts:\n",
    "                    max_counts[ngram] = max(max_counts[ngram], ref_counts[ngram])\n",
    "            \n",
    "            clipped_counts = {ngram: min(count, max_counts[ngram]) \n",
    "                            for ngram, count in gen_counts.items()}\n",
    "            \n",
    "            if gen_counts:\n",
    "                precision = sum(clipped_counts.values()) / sum(gen_counts.values())\n",
    "                precisions.append(precision)\n",
    "            else:\n",
    "                precisions.append(0.0)\n",
    "        \n",
    "        # Calculate brevity penalty\n",
    "        gen_length = len(word_tokenize(generated_text))\n",
    "        ref_lengths = [len(word_tokenize(ref)) for ref in reference_texts]\n",
    "        closest_ref_length = min(ref_lengths, key=lambda x: abs(x - gen_length))\n",
    "        \n",
    "        if gen_length > closest_ref_length:\n",
    "            bp = 1.0\n",
    "        else:\n",
    "            bp = math.exp(1 - closest_ref_length / gen_length) if gen_length > 0 else 0.0\n",
    "        \n",
    "        # Calculate BLEU score\n",
    "        if all(p > 0 for p in precisions):\n",
    "            geometric_mean = math.exp(sum(math.log(p) for p in precisions) / len(precisions))\n",
    "            bleu = bp * geometric_mean\n",
    "        else:\n",
    "            bleu = 0.0\n",
    "        \n",
    "        return bleu\n",
    "    \n",
    "    def calculate_rouge_l(self, generated_text: str, reference_text: str) -> float:\n",
    "        \"\"\"Calculate ROUGE-L score (Longest Common Subsequence)\"\"\"\n",
    "        def lcs_length(seq1, seq2):\n",
    "            m, n = len(seq1), len(seq2)\n",
    "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "            \n",
    "            for i in range(1, m + 1):\n",
    "                for j in range(1, n + 1):\n",
    "                    if seq1[i-1] == seq2[j-1]:\n",
    "                        dp[i][j] = dp[i-1][j-1] + 1\n",
    "                    else:\n",
    "                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "            \n",
    "            return dp[m][n]\n",
    "        \n",
    "        gen_tokens = word_tokenize(generated_text.lower())\n",
    "        ref_tokens = word_tokenize(reference_text.lower())\n",
    "        \n",
    "        if not gen_tokens or not ref_tokens:\n",
    "            return 0.0\n",
    "        \n",
    "        lcs_len = lcs_length(gen_tokens, ref_tokens)\n",
    "        \n",
    "        precision = lcs_len / len(gen_tokens)\n",
    "        recall = lcs_len / len(ref_tokens)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        return f1\n",
    "    \n",
    "    def calculate_distinct_ngrams(self, text: str, n: int = 2) -> float:\n",
    "        \"\"\"Calculate distinct n-gram ratio (diversity metric)\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        if len(tokens) < n:\n",
    "            return 0.0\n",
    "        \n",
    "        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "        unique_ngrams = set(ngrams)\n",
    "        \n",
    "        return len(unique_ngrams) / len(ngrams) if ngrams else 0.0\n",
    "    \n",
    "    def calculate_repetition_ratio(self, text: str, n: int = 4) -> float:\n",
    "        \"\"\"Calculate repetition ratio (lower is better)\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        if len(tokens) < n:\n",
    "            return 0.0\n",
    "        \n",
    "        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "        ngram_counts = Counter(ngrams)\n",
    "        \n",
    "        repeated_ngrams = sum(count - 1 for count in ngram_counts.values() if count > 1)\n",
    "        total_ngrams = len(ngrams)\n",
    "        \n",
    "        return repeated_ngrams / total_ngrams if total_ngrams > 0 else 0.0\n",
    "    \n",
    "    def calculate_semantic_coherence(self, text: str) -> float:\n",
    "        \"\"\"Calculate semantic coherence using sentence similarity\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        if len(sentences) < 2:\n",
    "            return 1.0  # Single sentence is coherent by definition\n",
    "        \n",
    "        # Simple TF-IDF based similarity\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        \n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(stop_words='english')\n",
    "            tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "            \n",
    "            similarities = []\n",
    "            for i in range(len(sentences) - 1):\n",
    "                sim = cosine_similarity(tfidf_matrix[i:i+1], tfidf_matrix[i+1:i+2])[0][0]\n",
    "                similarities.append(sim)\n",
    "            \n",
    "            return np.mean(similarities) if similarities else 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_text(self, generated_text: str, reference_texts: List[str] = None) -> Dict[str, float]:\n",
    "        \"\"\"Comprehensive evaluation of generated text\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Intrinsic metrics (don't need reference)\n",
    "        metrics['distinct_1'] = self.calculate_distinct_ngrams(generated_text, 1)\n",
    "        metrics['distinct_2'] = self.calculate_distinct_ngrams(generated_text, 2)\n",
    "        metrics['repetition_ratio'] = self.calculate_repetition_ratio(generated_text)\n",
    "        metrics['semantic_coherence'] = self.calculate_semantic_coherence(generated_text)\n",
    "        \n",
    "        # Length metrics\n",
    "        tokens = word_tokenize(generated_text)\n",
    "        metrics['length'] = len(tokens)\n",
    "        metrics['avg_word_length'] = np.mean([len(token) for token in tokens]) if tokens else 0\n",
    "        \n",
    "        # Reference-based metrics\n",
    "        if reference_texts:\n",
    "            bleu_scores = []\n",
    "            rouge_scores = []\n",
    "            \n",
    "            for ref_text in reference_texts:\n",
    "                bleu = self.calculate_bleu_score(generated_text, [ref_text])\n",
    "                rouge = self.calculate_rouge_l(generated_text, ref_text)\n",
    "                bleu_scores.append(bleu)\n",
    "                rouge_scores.append(rouge)\n",
    "            \n",
    "            metrics['bleu_avg'] = np.mean(bleu_scores)\n",
    "            metrics['rouge_l_avg'] = np.mean(rouge_scores)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def compare_models(self, model_outputs: Dict[str, List[str]], \n",
    "                      reference_texts: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Compare multiple models on text generation quality\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for model_name, generated_texts in model_outputs.items():\n",
    "            all_metrics = []\n",
    "            \n",
    "            for i, text in enumerate(generated_texts):\n",
    "                ref_texts = [reference_texts[i]] if reference_texts and i < len(reference_texts) else None\n",
    "                metrics = self.evaluate_text(text, ref_texts)\n",
    "                all_metrics.append(metrics)\n",
    "            \n",
    "            # Average metrics across all generated texts\n",
    "            avg_metrics = {'model': model_name}\n",
    "            if all_metrics:\n",
    "                for metric_name in all_metrics[0].keys():\n",
    "                    values = [m[metric_name] for m in all_metrics if metric_name in m]\n",
    "                    avg_metrics[f'avg_{metric_name}'] = np.mean(values) if values else 0.0\n",
    "            \n",
    "            results.append(avg_metrics)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = TextGenerationEvaluator()\n",
    "\n",
    "# Generate sample texts from different models for comparison\n",
    "print(\"\\nGenerating sample texts for evaluation...\")\n",
    "\n",
    "# Generate texts from different models\n",
    "model_outputs = {\n",
    "    'N-gram (3)': [],\n",
    "    'RNN': [],\n",
    "    'Transformer': []\n",
    "}\n",
    "\n",
    "prompts = [\"the future\", \"success is\", \"knowledge\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    # N-gram model\n",
    "    ngram_text = models[3].generate_text(prompt=prompt, max_length=20, temperature=1.0)\n",
    "    model_outputs['N-gram (3)'].append(ngram_text)\n",
    "    \n",
    "    # RNN model\n",
    "    rnn_text = trainer.generate_text(prompt=prompt, max_length=20, temperature=1.0)\n",
    "    model_outputs['RNN'].append(rnn_text)\n",
    "    \n",
    "    # Transformer model\n",
    "    transformer_text = transformer_trainer.generate_text(prompt=prompt, max_length=20, temperature=1.0)\n",
    "    model_outputs['Transformer'].append(transformer_text)\n",
    "\n",
    "# Evaluate and compare models\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_df = evaluator.compare_models(model_outputs)\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Detailed evaluation for one example\n",
    "print(\"\\nDetailed Evaluation Example:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "sample_text = model_outputs['Transformer'][0]\n",
    "detailed_metrics = evaluator.evaluate_text(sample_text)\n",
    "\n",
    "print(f\"Generated text: {sample_text}\")\n",
    "print(\"\\nMetrics:\")\n",
    "for metric, value in detailed_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "# Visualize some metrics\n",
    "if len(comparison_df) > 0:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Select interesting metrics for visualization\n",
    "    metrics_to_plot = ['avg_distinct_1', 'avg_distinct_2', 'avg_repetition_ratio', 'avg_semantic_coherence']\n",
    "    available_metrics = [m for m in metrics_to_plot if m in comparison_df.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        for i, metric in enumerate(available_metrics, 1):\n",
    "            plt.subplot(2, 2, i)\n",
    "            plt.bar(comparison_df['model'], comparison_df[metric])\n",
    "            plt.title(metric.replace('avg_', '').replace('_', ' ').title())\n",
    "            plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No suitable metrics found for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning Pre-trained Models\n",
    "\n",
    "Demonstrating how to fine-tune pre-trained language models for specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRANSFORMERS_AVAILABLE:\n",
    "    class FineTuningManager:\n",
    "        def __init__(self, model_name='gpt2', device='cpu'):\n",
    "            self.model_name = model_name\n",
    "            self.device = device\n",
    "            \n",
    "            # Load tokenizer and model\n",
    "            try:\n",
    "                self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "                self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "                \n",
    "                # Add padding token if not present\n",
    "                if self.tokenizer.pad_token is None:\n",
    "                    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                \n",
    "                self.model.to(device)\n",
    "                print(f\"Loaded {model_name} successfully!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {model_name}: {e}\")\n",
    "                self.tokenizer = None\n",
    "                self.model = None\n",
    "        \n",
    "        def prepare_dataset(self, texts: List[str], max_length: int = 128):\n",
    "            \"\"\"Prepare dataset for fine-tuning\"\"\"\n",
    "            if not self.tokenizer:\n",
    "                return None\n",
    "            \n",
    "            encodings = self.tokenizer(\n",
    "                texts,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            return encodings\n",
    "        \n",
    "        def fine_tune(self, train_texts: List[str], epochs: int = 3, lr: float = 5e-5, batch_size: int = 4):\n",
    "            \"\"\"Fine-tune the model on custom data\"\"\"\n",
    "            if not self.model or not self.tokenizer:\n",
    "                print(\"Model or tokenizer not available for fine-tuning.\")\n",
    "                return None\n",
    "            \n",
    "            print(f\"Fine-tuning {self.model_name} on {len(train_texts)} examples...\")\n",
    "            \n",
    "            # Prepare data\n",
    "            encodings = self.prepare_dataset(train_texts)\n",
    "            if encodings is None:\n",
    "                return None\n",
    "            \n",
    "            # Create dataset\n",
    "            class CustomDataset(torch.utils.data.Dataset):\n",
    "                def __init__(self, encodings):\n",
    "                    self.encodings = encodings\n",
    "                \n",
    "                def __getitem__(self, idx):\n",
    "                    return {key: tensor[idx] for key, tensor in self.encodings.items()}\n",
    "                \n",
    "                def __len__(self):\n",
    "                    return len(self.encodings['input_ids'])\n",
    "            \n",
    "            dataset = CustomDataset(encodings)\n",
    "            dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            # Setup optimizer\n",
    "            optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)\n",
    "            \n",
    "            # Training loop\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                \n",
    "                for batch in dataloader:\n",
    "                    # Move to device\n",
    "                    input_ids = batch['input_ids'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "                    loss = outputs.loss\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    epoch_loss += loss.item()\n",
    "                \n",
    "                avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "                total_loss += avg_epoch_loss\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}: Loss = {avg_epoch_loss:.4f}\")\n",
    "            \n",
    "            print(f\"Fine-tuning completed. Average loss: {total_loss / epochs:.4f}\")\n",
    "            return total_loss / epochs\n",
    "        \n",
    "        def generate_text(self, prompt: str, max_length: int = 50, temperature: float = 1.0,\n",
    "                         num_return_sequences: int = 1, do_sample: bool = True):\n",
    "            \"\"\"Generate text using the fine-tuned model\"\"\"\n",
    "            if not self.model or not self.tokenizer:\n",
    "                return [\"Model not available\"]\n",
    "            \n",
    "            self.model.eval()\n",
    "            \n",
    "            # Encode prompt\n",
    "            input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "            \n",
    "            # Generate\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids,\n",
    "                    max_length=len(input_ids[0]) + max_length,\n",
    "                    temperature=temperature,\n",
    "                    num_return_sequences=num_return_sequences,\n",
    "                    do_sample=do_sample,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode\n",
    "            generated_texts = []\n",
    "            for output in outputs:\n",
    "                text = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "                generated_texts.append(text)\n",
    "            \n",
    "            return generated_texts\n",
    "        \n",
    "        def save_model(self, save_path: str):\n",
    "            \"\"\"Save fine-tuned model\"\"\"\n",
    "            if self.model and self.tokenizer:\n",
    "                self.model.save_pretrained(save_path)\n",
    "                self.tokenizer.save_pretrained(save_path)\n",
    "                print(f\"Model saved to {save_path}\")\n",
    "        \n",
    "        def load_model(self, load_path: str):\n",
    "            \"\"\"Load fine-tuned model\"\"\"\n",
    "            try:\n",
    "                self.model = GPT2LMHeadModel.from_pretrained(load_path)\n",
    "                self.tokenizer = GPT2Tokenizer.from_pretrained(load_path)\n",
    "                self.model.to(self.device)\n",
    "                print(f\"Model loaded from {load_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load model: {e}\")\n",
    "    \n",
    "    # Example of fine-tuning (commented out to avoid downloading large models)\n",
    "    print(\"\\nFine-tuning Example (using smaller model):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # This would normally use a larger model like 'gpt2'\n",
    "    # For demo, we'll show the structure without actually running it\n",
    "    \n",
    "    fine_tuning_texts = [\n",
    "        \"The future of artificial intelligence is bright and full of possibilities.\",\n",
    "        \"Machine learning algorithms are revolutionizing how we process data.\",\n",
    "        \"Natural language processing enables computers to understand human language.\",\n",
    "        \"Deep learning networks can learn complex patterns from large datasets.\",\n",
    "        \"Technology is advancing at an unprecedented rate in the modern world.\"\n",
    "    ]\n",
    "    \n",
    "    # Uncomment the following lines to actually run fine-tuning:\n",
    "    # fine_tuner = FineTuningManager('gpt2', device)\n",
    "    # fine_tuner.fine_tune(fine_tuning_texts, epochs=2, lr=5e-5, batch_size=2)\n",
    "    # generated = fine_tuner.generate_text(\"The future of AI\", max_length=30)\n",
    "    # print(f\"Generated: {generated[0]}\")\n",
    "    \n",
    "    print(\"Fine-tuning code structure ready (commented out to avoid large downloads).\")\n",
    "    print(\"To use: uncomment the lines above and install transformers library.\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nTransformers library not available.\")\n",
    "    print(\"To use pre-trained models, install: pip install transformers\")\n",
    "    \n",
    "    # Alternative: Show how to adapt our custom models\n",
    "    print(\"\\nAlternative: Adapting Custom Models\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    class CustomModelAdapter:\n",
    "        def __init__(self, base_model, vocab):\n",
    "            self.base_model = base_model\n",
    "            self.vocab = vocab\n",
    "        \n",
    "        def adapt_to_domain(self, domain_texts: List[str], epochs: int = 5):\n",
    "            \"\"\"Adapt model to specific domain\"\"\"\n",
    "            print(f\"Adapting model to domain with {len(domain_texts)} examples...\")\n",
    "            \n",
    "            # Create domain-specific dataset\n",
    "            domain_dataset = TextDataset(domain_texts, self.vocab, sequence_length=20)\n",
    "            domain_loader = DataLoader(domain_dataset, batch_size=4, shuffle=True)\n",
    "            \n",
    "            if len(domain_dataset) == 0:\n",
    "                print(\"No valid sequences found in domain data.\")\n",
    "                return\n",
    "            \n",
    "            # Fine-tune with lower learning rate\n",
    "            trainer = NeuralLanguageModelTrainer(self.base_model, self.vocab, device)\n",
    "            losses, _ = trainer.train(domain_loader, epochs=epochs, lr=0.001)\n",
    "            \n",
    "            print(f\"Domain adaptation completed. Final loss: {losses[-1]:.4f}\")\n",
    "            return losses\n",
    "    \n",
    "    # Example domain adaptation\n",
    "    tech_domain_texts = [\n",
    "        \"Artificial intelligence is transforming industries worldwide.\",\n",
    "        \"Machine learning algorithms optimize business processes.\",\n",
    "        \"Cloud computing provides scalable infrastructure solutions.\",\n",
    "        \"Data science drives informed decision making.\"\n",
    "    ]\n",
    "    \n",
    "    adapter = CustomModelAdapter(model, vocab)\n",
    "    adaptation_losses = adapter.adapt_to_domain(tech_domain_texts, epochs=3)\n",
    "    \n",
    "    if adaptation_losses:\n",
    "        print(f\"\\nAdaptation completed with final loss: {adaptation_losses[-1]:.4f}\")\n",
    "        \n",
    "        # Test adapted model\n",
    "        adapted_text = trainer.generate_text(prompt=\"artificial intelligence\", max_length=15)\n",
    "        print(f\"Adapted model output: {adapted_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Language Model Challenges\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Challenge 1: Advanced N-gram Models\n",
    "Enhance the n-gram language model with better smoothing techniques and interpolation.\n",
    "\n",
    "**Requirements:**\n",
    "- Implement Good-Turing smoothing\n",
    "- Add interpolation between different n-gram orders\n",
    "- Support variable-length n-grams\n",
    "- Add vocabulary pruning based on frequency\n",
    "\n",
    "**Success Criteria:**\n",
    "- Improve perplexity by 15%+ over basic n-gram\n",
    "- Support n-grams from 1 to 5\n",
    "- Handle out-of-vocabulary words gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 1\n",
    "class AdvancedNGramModel:\n",
    "    def __init__(self, max_n=4, interpolation_weights=None):\n",
    "        # TODO: Initialize advanced n-gram model\n",
    "        self.max_n = max_n\n",
    "        self.models = {}  # Store models for each n\n",
    "        self.interpolation_weights = interpolation_weights or [0.1, 0.2, 0.3, 0.4]\n",
    "    \n",
    "    def good_turing_smoothing(self, counts):\n",
    "        # TODO: Implement Good-Turing smoothing\n",
    "        pass\n",
    "    \n",
    "    def interpolate_probabilities(self, context, word):\n",
    "        # TODO: Interpolate probabilities from different n-gram orders\n",
    "        pass\n",
    "    \n",
    "    def prune_vocabulary(self, min_freq=2):\n",
    "        # TODO: Remove low-frequency words\n",
    "        pass\n",
    "    \n",
    "    def train_hierarchical(self, texts):\n",
    "        # TODO: Train multiple n-gram models and combine\n",
    "        pass\n",
    "\n",
    "# TODO: Test your advanced n-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Controllable Text Generation\n",
    "Build a system that can generate text with specific attributes (sentiment, style, topic).\n",
    "\n",
    "**Requirements:**\n",
    "- Implement sentiment-controlled generation\n",
    "- Add style transfer capabilities\n",
    "- Support topic-guided generation\n",
    "- Create evaluation metrics for control\n",
    "\n",
    "**Success Criteria:**\n",
    "- Generate text with desired sentiment (85%+ accuracy)\n",
    "- Maintain fluency while controlling attributes\n",
    "- Support multiple control dimensions simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 2\n",
    "class ControllableTextGenerator:\n",
    "    def __init__(self, base_model):\n",
    "        # TODO: Initialize controllable generation system\n",
    "        self.base_model = base_model\n",
    "        self.sentiment_classifier = None\n",
    "        self.style_embeddings = None\n",
    "        self.topic_vectors = None\n",
    "    \n",
    "    def train_control_classifiers(self, training_data):\n",
    "        # TODO: Train classifiers for different attributes\n",
    "        pass\n",
    "    \n",
    "    def generate_with_sentiment(self, prompt, target_sentiment, strength=1.0):\n",
    "        # TODO: Generate text with specific sentiment\n",
    "        pass\n",
    "    \n",
    "    def generate_with_style(self, prompt, style_vector):\n",
    "        # TODO: Generate text in specific style\n",
    "        pass\n",
    "    \n",
    "    def generate_with_topic(self, prompt, topic_keywords):\n",
    "        # TODO: Generate text focused on specific topic\n",
    "        pass\n",
    "    \n",
    "    def evaluate_control_accuracy(self, generated_texts, target_attributes):\n",
    "        # TODO: Evaluate how well attributes are controlled\n",
    "        pass\n",
    "\n",
    "# TODO: Test controllable generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Challenge 3: Multi-Modal Language Models\n",
    "Build a language model that can incorporate visual information for image captioning and visual question answering.\n",
    "\n",
    "**Requirements:**\n",
    "- Integrate vision encoder with language model\n",
    "- Implement attention between vision and language\n",
    "- Support image captioning and VQA tasks\n",
    "- Handle multiple images in context\n",
    "\n",
    "**Success Criteria:**\n",
    "- Generate accurate image captions\n",
    "- Answer questions about image content\n",
    "- Maintain linguistic fluency with visual grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 3\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "class MultiModalLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, text_embed_dim=256, vision_embed_dim=2048):\n",
    "        super().__init__()\n",
    "        # TODO: Initialize multi-modal architecture\n",
    "        self.vision_encoder = None  # ResNet or ViT\n",
    "        self.text_encoder = None    # Transformer or RNN\n",
    "        self.fusion_layer = None    # Cross-modal attention\n",
    "        self.text_decoder = None    # Text generation head\n",
    "    \n",
    "    def encode_image(self, image):\n",
    "        # TODO: Extract visual features\n",
    "        pass\n",
    "    \n",
    "    def cross_modal_attention(self, vision_features, text_features):\n",
    "        # TODO: Implement attention between modalities\n",
    "        pass\n",
    "    \n",
    "    def forward(self, images, text_input):\n",
    "        # TODO: Forward pass for multi-modal input\n",
    "        pass\n",
    "\n",
    "class VisionLanguageTrainer:\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def train_image_captioning(self, image_caption_pairs):\n",
    "        # TODO: Train on image-caption pairs\n",
    "        pass\n",
    "    \n",
    "    def train_visual_qa(self, vqa_dataset):\n",
    "        # TODO: Train on VQA dataset\n",
    "        pass\n",
    "    \n",
    "    def generate_caption(self, image_path):\n",
    "        # TODO: Generate caption for image\n",
    "        pass\n",
    "    \n",
    "    def answer_visual_question(self, image_path, question):\n",
    "        # TODO: Answer question about image\n",
    "        pass\n",
    "\n",
    "# TODO: Test multi-modal language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: Memory-Augmented Language Models\n",
    "Implement language models with external memory mechanisms for long-context understanding.\n",
    "\n",
    "**Requirements:**\n",
    "- Implement differentiable memory bank\n",
    "- Add memory read/write operations\n",
    "- Support episodic and semantic memory\n",
    "- Handle long document understanding\n",
    "\n",
    "**Success Criteria:**\n",
    "- Maintain coherence over 1000+ token sequences\n",
    "- Demonstrate memory retrieval capabilities\n",
    "- Improve performance on long-context tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 4\n",
    "class MemoryAugmentedLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, memory_size=1000, memory_dim=512):\n",
    "        super().__init__()\n",
    "        # TODO: Initialize memory-augmented architecture\n",
    "        self.base_lm = None         # Base language model\n",
    "        self.memory_bank = None     # External memory\n",
    "        self.memory_controller = None # Read/write controller\n",
    "        self.attention_mechanism = None # Memory attention\n",
    "    \n",
    "    def initialize_memory(self, memory_size, memory_dim):\n",
    "        # TODO: Initialize memory bank\n",
    "        pass\n",
    "    \n",
    "    def read_memory(self, query):\n",
    "        # TODO: Read from memory using attention\n",
    "        pass\n",
    "    \n",
    "    def write_memory(self, key, value):\n",
    "        # TODO: Write to memory\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_ids, update_memory=True):\n",
    "        # TODO: Forward pass with memory operations\n",
    "        pass\n",
    "\n",
    "class MemoryTrainer:\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def train_long_context(self, long_documents):\n",
    "        # TODO: Train on long documents\n",
    "        pass\n",
    "    \n",
    "    def evaluate_memory_retrieval(self, test_cases):\n",
    "        # TODO: Test memory retrieval capabilities\n",
    "        pass\n",
    "    \n",
    "    def generate_with_memory(self, prompt, max_length=100):\n",
    "        # TODO: Generate using memory\n",
    "        pass\n",
    "\n",
    "# TODO: Test memory-augmented language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Challenge 5: Few-Shot Learning for Language Models\n",
    "Implement meta-learning approaches for few-shot adaptation of language models to new tasks.\n",
    "\n",
    "**Requirements:**\n",
    "- Implement MAML for language models\n",
    "- Support rapid adaptation to new tasks\n",
    "- Handle few-shot text classification and generation\n",
    "- Compare with prompt-based approaches\n",
    "\n",
    "**Success Criteria:**\n",
    "- Adapt to new tasks with 5-10 examples\n",
    "- Outperform standard fine-tuning in few-shot settings\n",
    "- Demonstrate transfer across diverse NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 5\n",
    "import copy\n",
    "\n",
    "class MAMLLanguageModel:\n",
    "    def __init__(self, base_model, vocab):\n",
    "        # TODO: Initialize MAML for language models\n",
    "        self.base_model = base_model\n",
    "        self.vocab = vocab\n",
    "        self.meta_lr = 1e-3\n",
    "        self.inner_lr = 1e-2\n",
    "        self.inner_steps = 5\n",
    "    \n",
    "    def create_task_batch(self, task_data, k_shot=5, q_query=10):\n",
    "        # TODO: Create support and query sets for meta-learning\n",
    "        pass\n",
    "    \n",
    "    def inner_update(self, model, support_data):\n",
    "        # TODO: Perform inner loop update\n",
    "        pass\n",
    "    \n",
    "    def meta_update(self, task_batch):\n",
    "        # TODO: Perform meta update across tasks\n",
    "        pass\n",
    "    \n",
    "    def meta_train(self, meta_train_tasks, episodes=1000):\n",
    "        # TODO: Meta-training loop\n",
    "        pass\n",
    "    \n",
    "    def few_shot_adapt(self, support_examples, task_type='classification'):\n",
    "        # TODO: Adapt to new task with few examples\n",
    "        pass\n",
    "\n",
    "class PromptBasedFewShot:\n",
    "    def __init__(self, base_model, vocab):\n",
    "        # TODO: Initialize prompt-based few-shot learning\n",
    "        self.base_model = base_model\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def create_prompt_template(self, task_type, examples):\n",
    "        # TODO: Create prompt from examples\n",
    "        pass\n",
    "    \n",
    "    def few_shot_inference(self, prompt, new_input):\n",
    "        # TODO: Perform few-shot inference via prompting\n",
    "        pass\n",
    "\n",
    "# TODO: Compare MAML vs prompt-based approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 6: Interpretable Language Models\n",
    "Build language models with built-in interpretability mechanisms to understand their decision-making process.\n",
    "\n",
    "**Requirements:**\n",
    "- Implement attention visualization\n",
    "- Add neuron activation analysis\n",
    "- Build causal intervention tools\n",
    "- Create concept discovery methods\n",
    "\n",
    "**Success Criteria:**\n",
    "- Identify important neurons for specific tasks\n",
    "- Demonstrate causal relationships in model behavior\n",
    "- Discover interpretable concepts in representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 6\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class InterpretableLanguageModel:\n",
    "    def __init__(self, base_model, vocab):\n",
    "        # TODO: Initialize interpretable LM\n",
    "        self.base_model = base_model\n",
    "        self.vocab = vocab\n",
    "        self.attention_weights = {}\n",
    "        self.neuron_activations = {}\n",
    "    \n",
    "    def register_hooks(self):\n",
    "        # TODO: Register hooks to capture activations\n",
    "        pass\n",
    "    \n",
    "    def visualize_attention(self, input_text, layer=0, head=0):\n",
    "        # TODO: Visualize attention patterns\n",
    "        pass\n",
    "    \n",
    "    def analyze_neuron_activations(self, input_texts, neuron_indices):\n",
    "        # TODO: Analyze specific neuron activations\n",
    "        pass\n",
    "    \n",
    "    def causal_intervention(self, input_text, intervention_layer, intervention_neurons):\n",
    "        # TODO: Perform causal interventions\n",
    "        pass\n",
    "    \n",
    "    def discover_concepts(self, concept_dataset):\n",
    "        # TODO: Discover interpretable concepts\n",
    "        pass\n",
    "    \n",
    "    def explain_prediction(self, input_text, prediction):\n",
    "        # TODO: Explain why model made specific prediction\n",
    "        pass\n",
    "\n",
    "class ModelProbing:\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def probe_syntactic_knowledge(self, probe_dataset):\n",
    "        # TODO: Probe for syntactic knowledge\n",
    "        pass\n",
    "    \n",
    "    def probe_semantic_knowledge(self, probe_dataset):\n",
    "        # TODO: Probe for semantic knowledge\n",
    "        pass\n",
    "    \n",
    "    def probe_world_knowledge(self, factual_dataset):\n",
    "        # TODO: Probe for world knowledge\n",
    "        pass\n",
    "\n",
    "# TODO: Test interpretability methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Bonus Challenge: Production Language Model Service**\n",
    "\n",
    "Build a complete production service for serving language models at scale.\n",
    "\n",
    "### Requirements:\n",
    "1. **Model Serving**: Multi-model serving with load balancing\n",
    "2. **Scalability**: Auto-scaling based on demand\n",
    "3. **Caching**: Intelligent caching for common queries\n",
    "4. **A/B Testing**: Compare different model versions\n",
    "5. **Monitoring**: Real-time performance monitoring\n",
    "6. **Safety**: Content filtering and bias detection\n",
    "7. **API Management**: Rate limiting and authentication\n",
    "8. **Model Updates**: Hot-swapping of models\n",
    "9. **Multi-tenancy**: Support multiple clients\n",
    "10. **Analytics**: Usage analytics and insights\n",
    "\n",
    "### Success Criteria:\n",
    "- Handle 10,000+ requests per minute\n",
    "- Sub-second response times for most queries\n",
    "- 99.99% uptime\n",
    "- Comprehensive monitoring dashboard\n",
    "- Automated deployment pipeline\n",
    "- Cost optimization features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Bonus Challenge\n",
    "from flask import Flask, request, jsonify\n",
    "import redis\n",
    "import asyncio\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "class ProductionLanguageModelService:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize production service\n",
    "        self.app = Flask(__name__)\n",
    "        self.models = {}  # Model registry\n",
    "        self.cache = redis.Redis()\n",
    "        self.executor = ThreadPoolExecutor(max_workers=10)\n",
    "        self.monitoring = {}\n",
    "        self.safety_filter = None\n",
    "        self.setup_routes()\n",
    "    \n",
    "    def setup_routes(self):\n",
    "        # TODO: Setup API routes\n",
    "        @self.app.route('/generate', methods=['POST'])\n",
    "        def generate_text():\n",
    "            # TODO: Text generation endpoint\n",
    "            pass\n",
    "        \n",
    "        @self.app.route('/models', methods=['GET'])\n",
    "        def list_models():\n",
    "            # TODO: List available models\n",
    "            pass\n",
    "        \n",
    "        @self.app.route('/health', methods=['GET'])\n",
    "        def health_check():\n",
    "            # TODO: Health check endpoint\n",
    "            pass\n",
    "    \n",
    "    def load_model(self, model_id, model_path):\n",
    "        # TODO: Load model into service\n",
    "        pass\n",
    "    \n",
    "    def intelligent_caching(self, request_hash, generation_params):\n",
    "        # TODO: Implement smart caching\n",
    "        pass\n",
    "    \n",
    "    def content_safety_filter(self, text):\n",
    "        # TODO: Filter inappropriate content\n",
    "        pass\n",
    "    \n",
    "    def rate_limit_check(self, client_id):\n",
    "        # TODO: Implement rate limiting\n",
    "        pass\n",
    "    \n",
    "    def log_request(self, request_data, response_data, latency):\n",
    "        # TODO: Log requests for analytics\n",
    "        pass\n",
    "    \n",
    "    def auto_scale(self):\n",
    "        # TODO: Auto-scaling logic\n",
    "        pass\n",
    "    \n",
    "    def a_b_test(self, request, models_to_test):\n",
    "        # TODO: A/B testing framework\n",
    "        pass\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self):\n",
    "        # TODO: Model lifecycle management\n",
    "        self.active_models = {}\n",
    "        self.model_versions = {}\n",
    "        self.deployment_queue = []\n",
    "    \n",
    "    def deploy_model(self, model_id, model_artifact):\n",
    "        # TODO: Deploy new model version\n",
    "        pass\n",
    "    \n",
    "    def rollback_model(self, model_id, target_version):\n",
    "        # TODO: Rollback to previous version\n",
    "        pass\n",
    "    \n",
    "    def hot_swap_model(self, model_id, new_model):\n",
    "        # TODO: Hot-swap model without downtime\n",
    "        pass\n",
    "\n",
    "class MonitoringSystem:\n",
    "    def __init__(self):\n",
    "        # TODO: Monitoring and alerting\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.alerts = []\n",
    "    \n",
    "    def track_latency(self, endpoint, latency):\n",
    "        # TODO: Track response latencies\n",
    "        pass\n",
    "    \n",
    "    def track_throughput(self, endpoint, timestamp):\n",
    "        # TODO: Track request throughput\n",
    "        pass\n",
    "    \n",
    "    def detect_anomalies(self):\n",
    "        # TODO: Anomaly detection\n",
    "        pass\n",
    "    \n",
    "    def generate_dashboard(self):\n",
    "        # TODO: Generate monitoring dashboard\n",
    "        pass\n",
    "\n",
    "# Docker and Kubernetes configuration\n",
    "dockerfile = \"\"\"\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 8000\n",
    "\n",
    "CMD [\"gunicorn\", \"--bind\", \"0.0.0.0:8000\", \"--workers\", \"4\", \"app:app\"]\n",
    "\"\"\"\n",
    "\n",
    "k8s_deployment = \"\"\"\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: language-model-service\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: language-model-service\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: language-model-service\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: lm-service\n",
    "        image: lm-service:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2000m\"\n",
    "            nvidia.com/gpu: 1\n",
    "          limits:\n",
    "            memory: \"8Gi\"\n",
    "            cpu: \"4000m\"\n",
    "            nvidia.com/gpu: 1\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Complete the production service implementation\n",
    "print(\"Production Language Model Service framework ready!\")\n",
    "print(\"Key components:\")\n",
    "print(\"- Multi-model serving with load balancing\")\n",
    "print(\"- Intelligent caching and rate limiting\")\n",
    "print(\"- Real-time monitoring and alerting\")\n",
    "print(\"- A/B testing and model management\")\n",
    "print(\"- Content safety and bias detection\")\n",
    "print(\"- Auto-scaling and cost optimization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}